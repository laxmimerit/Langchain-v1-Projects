{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3c364c80",
   "metadata": {},
   "source": [
    "### Langchain Agent in V1\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "```\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "now ->\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750be741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using Ollama qwen3 + nomic-embed-text\n",
      "‚úì Connected to existing Chroma vector store\n"
     ]
    }
   ],
   "source": [
    "# use the langchain pre-release for latest features\n",
    "# pip install --pre -U langchain langchain-ollama, langchain-chroma, langchain-core langchain-community\n",
    "\n",
    "# Required: pip install langchain langchain-ollama, langchain-chroma, langchain-core langchain-community\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# LLM & VECTOR STORE SETUP\n",
    "# ============================================================================\n",
    "# Ollama LLM with Qwen3\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3\", \n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Ollama Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Chroma Vector Store (assumes data already exists from previous code)\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"..\\\\01 Semantic Search\\\\chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Using Ollama qwen3 + nomic-embed-text\")\n",
    "print(\"‚úì Connected to existing Chroma vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ce7c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user said \"Hello\". I should respond in a friendly and welcoming manner. Let me make sure to acknowledge their greeting and offer assistance. Maybe add an emoji to keep it approachable. I should keep it simple and open-ended so they feel comfortable asking anything. Let me check for any typos or errors. Alright, that should work.\\n</think>\\n\\nHello! üòä How can I assist you today? I\\'m here to help with any questions or tasks you might have!', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T12:11:43.0938081Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2426048700, 'load_duration': 1615341000, 'prompt_eval_count': 9, 'prompt_eval_duration': 189843100, 'eval_count': 101, 'eval_duration': 616979300, 'model_name': 'qwen3'}, id='run--6bdaf00b-ec50-429e-be3e-0534a7990cd0-0', usage_metadata={'input_tokens': 9, 'output_tokens': 101, 'total_tokens': 110})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53deb9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Vector Store Connection...\n",
      "‚úì Collection 'pdf_collection' found with 593 documents\n",
      "\n",
      "‚úì Sample search for 'methodology':\n",
      "  1. Page 59: involves providing accurate information and transparency about the model‚Äôs uncertainty and\n",
      "limitatio...\n",
      "  2. Page 60: This method could lead to biased or incomplete feedback due to the diversity of opinions\n",
      "among label...\n",
      "  3. Page 103: Method Non-football (DROP) Football (DROP) GSM8k\n",
      "Zero-Shot 43.86 51.77 16.38\n",
      "Standard Prompting 58.7...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VECTOR STORE VERIFICATION\n",
    "# ============================================================================\n",
    "print(\"\\nüîç Testing Vector Store Connection...\")\n",
    "\n",
    "# Test 1: Check collection info\n",
    "collection = vector_store._collection\n",
    "doc_count = collection.count()\n",
    "print(f\"‚úì Collection '{collection.name}' found with {doc_count} documents\")\n",
    "\n",
    "\n",
    "# Test 2: Sample similarity search\n",
    "test_query = \"methodology\"\n",
    "results = vector_store.similarity_search(test_query, k=3)\n",
    "print(f\"\\n‚úì Sample search for '{test_query}':\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. Page {doc.metadata.get('page', '?')}: {doc.page_content[:100]}...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c7c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc82f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RETRIEVAL TOOL\n",
    "# ============================================================================\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve relevant information from the document to answer the query.\"\"\"\n",
    "    print(f\"üîç Searching: '{query}'\")\n",
    "    \n",
    "    # Perform similarity search\n",
    "    docs = vector_store.similarity_search(query, k=4)\n",
    "    \n",
    "    # Format for LLM\n",
    "    content = \"\\n\\n\".join(\n",
    "        f\"Page {doc.metadata.get('page', '?')}: {doc.page_content}\" \n",
    "        for doc in docs\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Found {len(docs)} relevant chunks\")\n",
    "    return content, docs\n",
    "\n",
    "# ============================================================================\n",
    "# AGENT CREATION\n",
    "# ============================================================================\n",
    "# Tools for the agent (single tool only)\n",
    "tools = [retrieve_context]\n",
    "\n",
    "# Agent prompt - simplified for single tool\n",
    "prompt = \"\"\"You are a research assistant with a document retrieval tool.\n",
    "\n",
    "Tool:\n",
    "- retrieve_context: Search the document for relevant information\n",
    "\n",
    "Always use the tool to find relevant information before answering.\n",
    "Cite page numbers and be thorough.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717c7717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000167AB7DED90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the agentic RAG\n",
    "rag_agent = create_agent(llm, tools, prompt=prompt)\n",
    "\n",
    "rag_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4810cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching: 'What are the main methods mentioned in this paper?'\n",
      "‚úì Found 4 relevant chunks\n"
     ]
    }
   ],
   "source": [
    "rag_agent\n",
    "\n",
    "response = rag_agent.invoke({'messages': \"What are the main methods mentioned in this paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bd0b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are the main methods mentioned in this paper?', additional_kwargs={}, response_metadata={}, id='a1b8972c-4fa3-46da-bfe1-9a53858018fb'),\n",
       "  AIMessage(content=\"<think>\\nOkay, the user is asking about the main methods mentioned in the paper. Since I need to use the retrieve_context tool, I should start by calling that function with the query specifically asking for the main methods. The tool will search the document and return relevant information. I should make sure to cite page numbers and be thorough in the answer. Let me check if there are any other parameters needed, but the function only requires the query. Alright, I'll structure the tool call correctly.\\n</think>\\n\\n\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T12:19:59.2848275Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2665051100, 'load_duration': 1627213300, 'prompt_eval_count': 194, 'prompt_eval_duration': 204301000, 'eval_count': 130, 'eval_duration': 808121100, 'model_name': 'qwen3'}, id='run--d0e7b77c-35d2-4181-aaf9-af4de9351af4-0', tool_calls=[{'type': 'tool_call', 'id': 'f3b31897-98a2-4bc2-9a98-334fcc1230ca', 'name': 'retrieve_context', 'args': {'query': 'What are the main methods mentioned in this paper?'}}], usage_metadata={'input_tokens': 194, 'output_tokens': 130, 'total_tokens': 324}),\n",
       "  ToolMessage(content='Page 103: Method Non-football (DROP) Football (DROP) GSM8k\\nZero-Shot 43.86 51.77 16.38\\nStandard Prompting 58.78 62.73 17.06\\nChain-of-Thought 74.77 59.56 60.87\\nLeast-to-Most 82.45 73.42 62.39\\nTable 36: Accuracies of different prompting methods on GSM8k and DROP benchmarks. Source:\\nZhou et al. [244]\\nDECOMP is a text-based prompting strategy that decomposes complex tasks into simpler\\nsubtasks and generates a plan to solve the task, similar to Least-to-Most prompting. The\\ncore idea of Decomposed Prompting involves dividing a complex task into multiple simpler\\nsubtasks. Each subtask is addressed separately using LLMs, and their results are then combined\\nto produce the final outcome. Tasks are decomposed based on their inherent structure. For\\ninstance, a question-answering task might be split into subtasks involving information retrieval,\\ncomprehension, and synthesis. The model can process each step more effectively by focusing\\non these individual components.\\nFigure 48: The DECOMP framework. Source: Khot et al. [175]\\n\\nPage 4: the transformative impact of LLMs across various domains, including healthcare, finance,\\neducation, law, and scientific research.\\n‚Ä¢ Section 3 focuses on the fundamental building blocks of LLMs, covering data preprocess-\\ning techniques, pre-training methodologies, and model adaptation strategies. It explores\\nvarious pre-training approaches, including unsupervised, supervised, and semi-supervised\\nlearning, emphasizing their impact on model performance and adaptability. The section\\nalso examines different data sources used in LLM training, categorizing them into gen-\\neral data like Web pages, books, and conversation text, specialized data such as scientific\\nliterature and code, and widely used datasets like Wikipedia, BookCorpus, and Com-\\nmonCrawl. It details the critical data preprocessing steps, such as quality filtering, data\\ncleaning, deduplication, and tokenization, and their role in preparing data for effective\\nLLM training. Moreover, it discusses model adaptation techniques like instruction tuning\\n\\nPage 95: Taylor need to do before this? A. get a certificate , B. teach\\nsmall children, C. work in a school\\nARC Science Choose your answer to the question: Which technology was\\ndeveloped most recently? A. cellular telephone, B. television,\\nC. refrigerator, D. airplane\\nQASC Science Choose your answer to the question: What is described in terms\\nof temperature and water in the air? A. storms; B. climate;\\nC. mass; D. seasonal; E. winter; F. density; G. length\\nHellaSWAG Event Choose your answer to the question: We see a chair with a pillow\\non it. A. a man holding a cat does curling. B. a man holding a\\ncat starts hitting objects on an item. C. a man holding a cat is\\nwrapping a box. D. a man holding a cat sits down on the\\nchair.\\nNumerSense Numerical a square is a shape with ‚å©mask‚å™equally length sides. (four)\\nProtoQA Prototypical Use simple words separated by commas to name something in\\nyour life that could cause you to lose weight. ( Eating less,\\nexercising more, stress.)\\n\\nPage 83: Figure 36: Taxonomy of in-context learning. The training and the inference stage are two main\\nstages for ICL. During the training stage, existing ICL studies mainly take a pre-trained LLM as the\\nbackbone and optionally warm up the model to strengthen and generalize the ICL ability. Towards\\nthe inference stage, the demonstration design and the scoring function selection are crucial for the\\nultimate performance. Source: Dong et al. [265]\\nDesigning branch). The selection aims to choose good examples for ICL using unsupervised 72\\nor supervised methods. For example, KATE [186] and EPR [208] select demonstrations based\\non similarity. Ordering the selected demonstrations is also an important aspect of demonstra-\\ntion design. Lu et al. [190] have proven that order sensitivity is a common problem and affects\\nvarious models. To address this problem, studies have proposed several training-free meth-\\nods for ordering demonstrations. Liu et al. [186] sorted examples based on similarity, while', name='retrieve_context', id='488111b4-6aed-4c56-bcac-5bdad815427d', tool_call_id='f3b31897-98a2-4bc2-9a98-334fcc1230ca', artifact=[Document(id='600b2c00-838b-47a4-9f3d-12e54d5c1373', metadata={'moddate': '2025-02-11T01:48:37+00:00', 'keywords': '', 'creationdate': '2025-02-11T01:48:37+00:00', 'subject': '', 'page_label': '104', 'total_pages': 174, 'page': 103, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'author': '', 'title': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'start_index': 0}, page_content='Method Non-football (DROP) Football (DROP) GSM8k\\nZero-Shot 43.86 51.77 16.38\\nStandard Prompting 58.78 62.73 17.06\\nChain-of-Thought 74.77 59.56 60.87\\nLeast-to-Most 82.45 73.42 62.39\\nTable 36: Accuracies of different prompting methods on GSM8k and DROP benchmarks. Source:\\nZhou et al. [244]\\nDECOMP is a text-based prompting strategy that decomposes complex tasks into simpler\\nsubtasks and generates a plan to solve the task, similar to Least-to-Most prompting. The\\ncore idea of Decomposed Prompting involves dividing a complex task into multiple simpler\\nsubtasks. Each subtask is addressed separately using LLMs, and their results are then combined\\nto produce the final outcome. Tasks are decomposed based on their inherent structure. For\\ninstance, a question-answering task might be split into subtasks involving information retrieval,\\ncomprehension, and synthesis. The model can process each step more effectively by focusing\\non these individual components.\\nFigure 48: The DECOMP framework. Source: Khot et al. [175]'), Document(id='02323f2f-d907-4229-ba40-9e7136f65dc3', metadata={'total_pages': 174, 'trapped': '/False', 'keywords': '', 'title': '', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'moddate': '2025-02-11T01:48:37+00:00', 'page_label': '5', 'start_index': 899, 'author': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'page': 4, 'creationdate': '2025-02-11T01:48:37+00:00', 'creator': 'LaTeX with hyperref'}, page_content='the transformative impact of LLMs across various domains, including healthcare, finance,\\neducation, law, and scientific research.\\n‚Ä¢ Section 3 focuses on the fundamental building blocks of LLMs, covering data preprocess-\\ning techniques, pre-training methodologies, and model adaptation strategies. It explores\\nvarious pre-training approaches, including unsupervised, supervised, and semi-supervised\\nlearning, emphasizing their impact on model performance and adaptability. The section\\nalso examines different data sources used in LLM training, categorizing them into gen-\\neral data like Web pages, books, and conversation text, specialized data such as scientific\\nliterature and code, and widely used datasets like Wikipedia, BookCorpus, and Com-\\nmonCrawl. It details the critical data preprocessing steps, such as quality filtering, data\\ncleaning, deduplication, and tokenization, and their role in preparing data for effective\\nLLM training. Moreover, it discusses model adaptation techniques like instruction tuning'), Document(id='12665abf-e4df-4e93-9090-37a5b6882bbe', metadata={'subject': '', 'moddate': '2025-02-11T01:48:37+00:00', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creationdate': '2025-02-11T01:48:37+00:00', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'page_label': '96', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'author': '', 'page': 95, 'total_pages': 174, 'start_index': 894}, page_content='Taylor need to do before this? A. get a certificate , B. teach\\nsmall children, C. work in a school\\nARC Science Choose your answer to the question: Which technology was\\ndeveloped most recently? A. cellular telephone, B. television,\\nC. refrigerator, D. airplane\\nQASC Science Choose your answer to the question: What is described in terms\\nof temperature and water in the air? A. storms; B. climate;\\nC. mass; D. seasonal; E. winter; F. density; G. length\\nHellaSWAG Event Choose your answer to the question: We see a chair with a pillow\\non it. A. a man holding a cat does curling. B. a man holding a\\ncat starts hitting objects on an item. C. a man holding a cat is\\nwrapping a box. D. a man holding a cat sits down on the\\nchair.\\nNumerSense Numerical a square is a shape with ‚å©mask‚å™equally length sides. (four)\\nProtoQA Prototypical Use simple words separated by commas to name something in\\nyour life that could cause you to lose weight. ( Eating less,\\nexercising more, stress.)'), Document(id='1808df2f-f429-4ccd-9284-bb494e751617', metadata={'subject': '', 'total_pages': 174, 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'creator': 'LaTeX with hyperref', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': '2025-02-11T01:48:37+00:00', 'page': 83, 'creationdate': '2025-02-11T01:48:37+00:00', 'keywords': '', 'page_label': '84', 'producer': 'pdfTeX-1.40.25', 'author': '', 'trapped': '/False', 'title': '', 'start_index': 1025}, page_content='Figure 36: Taxonomy of in-context learning. The training and the inference stage are two main\\nstages for ICL. During the training stage, existing ICL studies mainly take a pre-trained LLM as the\\nbackbone and optionally warm up the model to strengthen and generalize the ICL ability. Towards\\nthe inference stage, the demonstration design and the scoring function selection are crucial for the\\nultimate performance. Source: Dong et al. [265]\\nDesigning branch). The selection aims to choose good examples for ICL using unsupervised 72\\nor supervised methods. For example, KATE [186] and EPR [208] select demonstrations based\\non similarity. Ordering the selected demonstrations is also an important aspect of demonstra-\\ntion design. Lu et al. [190] have proven that order sensitivity is a common problem and affects\\nvarious models. To address this problem, studies have proposed several training-free meth-\\nods for ordering demonstrations. Liu et al. [186] sorted examples based on similarity, while')]),\n",
       "  AIMessage(content=\"<think>\\nOkay, the user is asking about the main methods mentioned in the paper. Let me look through the retrieved context to find relevant information.\\n\\nFirst, on page 103, there's a table comparing different prompting methods like Zero-Shot, Standard Prompting, Chain-of-Thought, and Least-to-Most. These are methods used in prompting strategies for models. Then, on page 4, there's a mention of DECOMP, which is a text-based prompting strategy that decomposes tasks into subtasks. The description explains that DECOMP involves breaking down complex tasks into simpler subtasks and using LLMs to address each part. \\n\\nLooking at page 95, there are examples of different tasks like ARC Science, QASC Science, HellaSWAG, NumerSense, and ProtoQA. These seem to be specific tasks or benchmarks used to evaluate models, but maybe they're part of the methods discussed. However, the user is asking about methods, not tasks. \\n\\nOn page 83, there's a discussion about in-context learning (ICL), mentioning training and inference stages. The text talks about demonstration design and scoring functions, as well as methods like KATE and EPR for selecting examples based on similarity. Also, Liu et al. [186] sorted examples by similarity. \\n\\nPutting this together, the main methods mentioned are various prompting strategies (Zero-Shot, Standard Prompting, Chain-of-Thought, Least-to-Most, DECOMP) and in-context learning techniques (ICL) with specific approaches like demonstration selection and ordering. The DECOMP method is detailed as decomposing tasks into subtasks, and ICL involves demonstration design and scoring functions. The KATE and EPR methods are part of the ICL framework for selecting examples. \\n\\nI need to make sure I'm not missing any other methods. The user might be looking for the primary techniques the paper discusses, so I should list these methods with their descriptions and citations where available.\\n</think>\\n\\nThe paper mentions several key methods, primarily focusing on prompting strategies and in-context learning (ICL) techniques. Here's a structured summary:\\n\\n1. **Prompting Methods**  \\n   - **Zero-Shot Prompting**: Achieved 43.86% accuracy on GSM8k (Page 103).  \\n   - **Standard Prompting**: Baseline method with 58.78% accuracy (Page 103).  \\n   - **Chain-of-Thought (CoT)**: Improved performance to 74.77% accuracy (Page 103).  \\n   - **Least-to-Most Prompting**: A step-by-step decomposition method, achieving 82.45% accuracy (Page 103).  \\n   - **DECOMP (Decomposed Prompting)**: A text-based strategy that breaks complex tasks into simpler subtasks (Page 4). For example, a question-answering task might split into retrieval, comprehension, and synthesis subtasks. The method emphasizes structured decomposition and combining subtask results (Page 4).  \\n\\n2. **In-Context Learning (ICL) Techniques**  \\n   - **Training Stage**: Pre-trained LLMs are warmed up to enhance ICL capabilities (Page 83).  \\n   - **Demonstration Design**: Critical for inference performance, involving example selection and ordering. Methods like **KATE** [186] and **EPR** [208] select demonstrations based on similarity.  \\n   - **Ordering Demonstrations**: Addressing order sensitivity, methods such as sorting examples by similarity (e.g., Liu et al. [186]) are proposed to improve consistency (Page 83).  \\n\\n3. **Task-Specific Methods**  \\n   - **ARC Science, QASC Science, HellaSWAG, NumerSense, ProtoQA**: These are benchmark tasks or datasets used to evaluate model performance, though they may not represent standalone methods (Page 95).  \\n\\nFor further details, refer to the cited works (e.g., Zhou et al. [244], Khot et al. [175], Dong et al. [265]).\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T12:20:07.7491315Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5746623500, 'load_duration': 184128400, 'prompt_eval_count': 1293, 'prompt_eval_duration': 114589600, 'eval_count': 862, 'eval_duration': 5429812800, 'model_name': 'qwen3'}, id='run--63a01e69-ff59-4eed-9fd2-0a7f0a46bcb9-0', usage_metadata={'input_tokens': 1293, 'output_tokens': 862, 'total_tokens': 2155})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f3bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUERY FUNCTION\n",
    "# ============================================================================\n",
    "def ask(question: str):\n",
    "    \"\"\"Ask the agentic RAG a question.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for event in rag_agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        msg = event[\"messages\"][-1]\n",
    "        \n",
    "        # Show tool usage\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"\\nüîß Using: {tc['name']} with {tc['args']}\")\n",
    "        \n",
    "        # Show final answer\n",
    "        elif hasattr(msg, 'content') and msg.content:\n",
    "            print(f\"\\nüí¨ Answer:\\n{msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed913f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: What are the main methods mentioned in this paper?\n",
      "============================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "What are the main methods mentioned in this paper?\n",
      "\n",
      "üîß Using: retrieve_context with {'query': 'What are the main methods mentioned in this paper?'}\n",
      "üîç Searching: 'What are the main methods mentioned in this paper?'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "üí¨ Answer:\n",
      "Page 103: Method Non-football (DROP) Football (DROP) GSM8k\n",
      "Zero-Shot 43.86 51.77 16.38\n",
      "Standard Prompting 58.78 62.73 17.06\n",
      "Chain-of-Thought 74.77 59.56 60.87\n",
      "Least-to-Most 82.45 73.42 62.39\n",
      "Table 36: Accuracies of different prompting methods on GSM8k and DROP benchmarks. Source:\n",
      "Zhou et al. [244]\n",
      "DECOMP is a text-based prompting strategy that decomposes complex tasks into simpler\n",
      "subtasks and generates a plan to solve the task, similar to Least-to-Most prompting. The\n",
      "core idea of Decomposed Prompting involves dividing a complex task into multiple simpler\n",
      "subtasks. Each subtask is addressed separately using LLMs, and their results are then combined\n",
      "to produce the final outcome. Tasks are decomposed based on their inherent structure. For\n",
      "instance, a question-answering task might be split into subtasks involving information retrieval,\n",
      "comprehension, and synthesis. The model can process each step more effectively by focusing\n",
      "on these individual components.\n",
      "Figure 48: The DECOMP framework. Source: Khot et al. [175]\n",
      "\n",
      "Page 4: the transformative impact of LLMs across various domains, including healthcare, finance,\n",
      "education, law, and scientific research.\n",
      "‚Ä¢ Section 3 focuses on the fundamental building blocks of LLMs, covering data preprocess-\n",
      "ing techniques, pre-training methodologies, and model adaptation strategies. It explores\n",
      "various pre-training approaches, including unsupervised, supervised, and semi-supervised\n",
      "learning, emphasizing their impact on model performance and adaptability. The section\n",
      "also examines different data sources used in LLM training, categorizing them into gen-\n",
      "eral data like Web pages, books, and conversation text, specialized data such as scientific\n",
      "literature and code, and widely used datasets like Wikipedia, BookCorpus, and Com-\n",
      "monCrawl. It details the critical data preprocessing steps, such as quality filtering, data\n",
      "cleaning, deduplication, and tokenization, and their role in preparing data for effective\n",
      "LLM training. Moreover, it discusses model adaptation techniques like instruction tuning\n",
      "\n",
      "Page 95: Taylor need to do before this? A. get a certificate , B. teach\n",
      "small children, C. work in a school\n",
      "ARC Science Choose your answer to the question: Which technology was\n",
      "developed most recently? A. cellular telephone, B. television,\n",
      "C. refrigerator, D. airplane\n",
      "QASC Science Choose your answer to the question: What is described in terms\n",
      "of temperature and water in the air? A. storms; B. climate;\n",
      "C. mass; D. seasonal; E. winter; F. density; G. length\n",
      "HellaSWAG Event Choose your answer to the question: We see a chair with a pillow\n",
      "on it. A. a man holding a cat does curling. B. a man holding a\n",
      "cat starts hitting objects on an item. C. a man holding a cat is\n",
      "wrapping a box. D. a man holding a cat sits down on the\n",
      "chair.\n",
      "NumerSense Numerical a square is a shape with ‚å©mask‚å™equally length sides. (four)\n",
      "ProtoQA Prototypical Use simple words separated by commas to name something in\n",
      "your life that could cause you to lose weight. ( Eating less,\n",
      "exercising more, stress.)\n",
      "\n",
      "Page 83: Figure 36: Taxonomy of in-context learning. The training and the inference stage are two main\n",
      "stages for ICL. During the training stage, existing ICL studies mainly take a pre-trained LLM as the\n",
      "backbone and optionally warm up the model to strengthen and generalize the ICL ability. Towards\n",
      "the inference stage, the demonstration design and the scoring function selection are crucial for the\n",
      "ultimate performance. Source: Dong et al. [265]\n",
      "Designing branch). The selection aims to choose good examples for ICL using unsupervised 72\n",
      "or supervised methods. For example, KATE [186] and EPR [208] select demonstrations based\n",
      "on similarity. Ordering the selected demonstrations is also an important aspect of demonstra-\n",
      "tion design. Lu et al. [190] have proven that order sensitivity is a common problem and affects\n",
      "various models. To address this problem, studies have proposed several training-free meth-\n",
      "ods for ordering demonstrations. Liu et al. [186] sorted examples based on similarity, while\n",
      "\n",
      "üí¨ Answer:\n",
      "<think>\n",
      "Okay, let's tackle the user's question about the main methods mentioned in the paper. First, I need to go through the provided tool response to find relevant information.\n",
      "\n",
      "Looking at the pages retrieved, there's a mention of different prompting methods on page 103, like Zero-Shot, Standard Prompting, Chain-of-Thought, and Least-to-Most. These are all methods used in prompting strategies for models. Then, on page 4, there's a section about the fundamental building blocks of LLMs, including data preprocessing, pre-training methodologies, and model adaptation strategies. That covers techniques like unsupervised, supervised, and semi-supervised learning, as well as data sources and preprocessing steps.\n",
      "\n",
      "On page 95, there are examples of tasks and questions, which might relate to the application of these methods in specific domains. However, the main methods seem to be the prompting strategies and the LLM building blocks. Additionally, page 83 talks about in-context learning (ICL), with stages like training and inference, and methods for demonstration design and scoring functions. There's also mention of techniques like KATE and EPR for selecting demonstrations based on similarity, and ordering demonstrations to address order sensitivity.\n",
      "\n",
      "Putting this together, the main methods would include the various prompting techniques (Zero-Shot, Chain-of-Thought, etc.), the pre-training approaches (unsupervised, supervised), data preprocessing steps, and the ICL methods with demonstration design and ordering. The DECOMP framework mentioned on page 4 is another method, which decomposes tasks into subtasks. I should make sure to list these clearly, citing the relevant pages.\n",
      "</think>\n",
      "\n",
      "The paper mentions several key methods across different contexts. Here's a structured summary:\n",
      "\n",
      "1. **Prompting Strategies** (Page 103):  \n",
      "   - **Zero-Shot Prompting**: Achieved 43.86% accuracy on GSM8k.  \n",
      "   - **Standard Prompting**: 58.78% accuracy.  \n",
      "   - **Chain-of-Thought (CoT)**: 74.77% accuracy.  \n",
      "   - **Least-to-Most Prompting**: 82.45% accuracy.  \n",
      "   - These methods focus on leveraging pre-trained models for task execution through varied prompting techniques.\n",
      "\n",
      "2. **Decomposed Prompting (DECOMP)** (Page 4):  \n",
      "   - A text-based strategy that breaks complex tasks into simpler subtasks (e.g., information retrieval, comprehension, synthesis).  \n",
      "   - Combines results from subtasks to produce final outputs.  \n",
      "   - Similar to Least-to-Most but emphasizes structured decomposition.\n",
      "\n",
      "3. **In-Context Learning (ICL)** (Page 83):  \n",
      "   - **Training Stage**: Uses pre-trained LLMs with optional warm-up for generalizing ICL abilities.  \n",
      "   - **Inference Stage**: Focuses on demonstration design (e.g., similarity-based selection via KATE [186], EPR [208]) and scoring functions.  \n",
      "   - **Order Sensitivity**: Addressed through training-free methods like Liu et al. [186]‚Äôs similarity-based ordering.\n",
      "\n",
      "4. **LLM Fundamentals** (Page 4):  \n",
      "   - **Pre-Training Approaches**: Unsupervised, supervised, and semi-supervised learning.  \n",
      "   - **Data Sources**: General (Web pages, books) and specialized (scientific literature, code).  \n",
      "   - **Preprocessing**: Quality filtering, cleaning, deduplication, and tokenization.  \n",
      "   - **Adaptation Techniques**: Instruction tuning for model customization.\n",
      "\n",
      "5. **Task-Specific Applications** (Page 95):  \n",
      "   - Examples include science reasoning (ARC Science, QASC Science), commonsense reasoning (HellaSWAG), numerical reasoning (NumerSense), and prototypical QA (ProtoQA).  \n",
      "\n",
      "These methods collectively highlight the paper‚Äôs focus on improving LLM performance through advanced prompting, decomposition, and training strategies.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "# Test basic retrieval\n",
    "ask(\"What are the main methods mentioned in this paper?\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a082a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agentic RAG Chat - Type 'quit' to exit\n",
      "\n",
      "============================================================\n",
      "Question: what are methods of RAG?\n",
      "============================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "what are methods of RAG?\n",
      "\n",
      "üîß Using: retrieve_context with {'query': 'methods of RAG'}\n",
      "üîç Searching: 'methods of RAG'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "üí¨ Answer:\n",
      "Page 122: Benchmarks such as SQuAD [33], Natural Questions [71], and specialized datasets for re-\n",
      "trieval tasks are widely used for assessment.\n",
      "Despite its promise, RAG faces several challenges:\n",
      "1. Retrieval Latency: Efficiently querying large databases in real time remains a technical\n",
      "hurdle.\n",
      "2. Data Quality: The reliability of generated outputs depends heavily on the quality of\n",
      "retrieved data.\n",
      "3. Scalability: Handling large-scale retrieval tasks while maintaining high generation quality\n",
      "is complex.\n",
      "Future research avenues include:\n",
      "‚Ä¢ Expanding RAG frameworks to support multi-modal inputs, such as text, images, and\n",
      "audio.\n",
      "‚Ä¢ Enhancing retrieval efficiency through novel indexing and search techniques.\n",
      "‚Ä¢ Improving integration mechanisms for tighter coupling between retrieval and generation\n",
      "modules.\n",
      "RAG represents a transformative step in LLM development, bridging the gap between static\n",
      "pre-trained knowledge and dynamic, context-aware generation. By combining retrieval and\n",
      "\n",
      "Page 122: Self-RAG [262] enable LLMs to determine optimal retrieval moments and content, im-\n",
      "proving the adaptive capabilities of RAG frameworks. GraphToolformer [291] divides\n",
      "retrieval into distinct stages, where LLMs actively utilize tools such as retrievers and\n",
      "apply techniques like Self-Ask or few-shot prompts to guide the process. WebGPT [124]\n",
      "integrates reinforcement learning to train LLMs for autonomous search engine usage. By\n",
      "leveraging special tokens for actions such as querying, browsing, and citing sources, it\n",
      "mimics an agent actively gathering and validating information during generation.\n",
      "Some of the most widely used metrics for evaluating RAG systems include:\n",
      "‚Ä¢ Retrieval Precision: Measures the relevance of retrieved data.\n",
      "‚Ä¢ Generation Accuracy: Assesses the factual correctness of outputs.\n",
      "‚Ä¢ End-to-End Performance: Evaluates the overall coherence, fluency, and informativeness\n",
      "of the system.\n",
      "Benchmarks such as SQuAD [33], Natural Questions [71], and specialized datasets for re-\n",
      "\n",
      "Page 116: 1. Enhanced Knowledge Integration : By querying external databases, RAG systems\n",
      "continuously update their knowledge base, addressing the limitations of static pre-trained\n",
      "models.\n",
      "2. Improved Accuracy: Retrieved data serves as contextual grounding, reducing halluci-\n",
      "nations and increasing the factual reliability of generated outputs.\n",
      "117\n",
      "\n",
      "Page 117: hance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\n",
      "like sliding window approach, fine-grained segmentation and metadata. It incorporates\n",
      "additional optimization techniques to streamline the retrieval process [280].\n",
      "3. Modular RAG: this architecture advances beyond previous RAG paradigms (Naive and\n",
      "Advanced RAG) by offering greater adaptability, flexibility, and functionality. It intro-\n",
      "duces new components and interaction patterns to address the challenges of static and\n",
      "rigid retrieval-generation frameworks, making it suitable for diverse tasks and dynamic\n",
      "scenarios. Modular RAG incorporates specialized modules to enhance retrieval and gen-\n",
      "eration:\n",
      "‚Ä¢ Search Module: Supports direct searches across diverse data sources such as databases,\n",
      "search engines, and knowledge graphs using LLM-generated queries [303].\n",
      "‚Ä¢ RAGFusion: Implements multi-query strategies for diverse perspectives, utilizing\n",
      "parallel searches and re-ranking for knowledge discovery [320].\n",
      "118\n",
      "\n",
      "üí¨ Answer:\n",
      "<think>\n",
      "Okay, the user asked about the methods of RAG. Let me go through the retrieved document to find relevant information.\n",
      "\n",
      "First, on page 116, it mentions Enhanced Knowledge Integration and Improved Accuracy. These are benefits, not methods, but they might relate to techniques used. Then, page 117 talks about indexing techniques like sliding window and fine-grained segmentation. That's a method for retrieval. \n",
      "\n",
      "Looking at page 122, there's mention of Self-RAG, GraphToolformer, and WebGPT. These seem like specific methods or frameworks. Also, the document discusses retrieval precision and generation accuracy as metrics, which might tie into evaluation methods. \n",
      "\n",
      "Modular RAG is mentioned on page 118, with components like Search Module and RAGFusion. These are architectural methods. The text also refers to techniques like Self-Ask and few-shot prompts, which are part of the retrieval process. \n",
      "\n",
      "I need to organize these points into clear methods. The answer should include retrieval techniques (dense vs. sparse), generation integration, modular architectures, and evaluation metrics. Make sure to cite the page numbers for each point.\n",
      "</think>\n",
      "\n",
      "The methods of Retrieval-Augmented Generation (RAG) involve integrating retrieval systems with language models to enhance generation quality. Key methods include:\n",
      "\n",
      "1. **Retrieval Techniques**  \n",
      "   - **Dense Retrieval**: Uses transformer-based models (e.g., BERT) to retrieve relevant documents based on semantic similarity [Page 117].  \n",
      "   - **Sparse Retrieval**: Relies on inverted indexes and keyword matching for faster queries [Page 117].  \n",
      "   - **Hybrid Approaches**: Combines dense and sparse methods for efficiency and accuracy [Page 117].  \n",
      "\n",
      "2. **Modular RAG Architecture**  \n",
      "   - **Search Module**: Enables direct searches across databases, search engines, and knowledge graphs using LLM-generated queries [Page 118].  \n",
      "   - **RAGFusion**: Implements multi-query strategies with parallel searches and re-ranking to gather diverse perspectives [Page 118].  \n",
      "\n",
      "3. **Interactive Retrieval-Generation Frameworks**  \n",
      "   - **Self-RAG**: Allows LLMs to dynamically decide when and what to retrieve, improving adaptability [Page 122].  \n",
      "   - **GraphToolformer**: Structures retrieval into stages, using tools like retrievers and techniques such as Self-Ask or few-shot prompts [Page 122].  \n",
      "   - **WebGPT**: Integrates reinforcement learning for autonomous search, using special tokens to simulate agent-like information gathering [Page 122].  \n",
      "\n",
      "4. **Evaluation Metrics**  \n",
      "   - **Retrieval Precision**: Measures the relevance of retrieved data [Page 122].  \n",
      "   - **Generation Accuracy**: Assesses factual correctness of outputs [Page 122].  \n",
      "   - **End-to-End Performance**: Evaluates coherence, fluency, and informativeness of the system [Page 122].  \n",
      "\n",
      "These methods address challenges like retrieval latency, data quality, and scalability while enhancing knowledge integration and reducing hallucinations [Page 116].\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE CHAT\n",
    "# ============================================================================\n",
    "def chat():\n",
    "    \"\"\"Start interactive chat with the agentic RAG.\"\"\"\n",
    "    print(\"\\nü§ñ Agentic RAG Chat - Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        if question:\n",
    "            ask(question)\n",
    "\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
