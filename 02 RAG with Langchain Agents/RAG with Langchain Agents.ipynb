{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAAAXNSR0IArs4c6QAAIABJREFUeAHtnYdfE0n7wH//0fvqqhHhjIIIiIgep1hOPct5dikJxSjSPDsqCUkghNBBEAQRRLFgQUXQ89R7BdIghdAJTUTQ/d1mwxIgxCRkJ7Nk+eyHz+yU55l5nm9mdmbL/B9K/9EWgMAC/wdBHegq0BZAaRBpCKCwAA0iFG6gK0GDSDMAhQVoEKFwA10JGkSaASgsQIM40w1dqq81BZ03r2vyzqty/mzNjG3NSjT8j2/NjFdip3GKrPhW/MiMw2Iy47BTLGCIz4xtlcQpjTFxSmMqXtaQTWIolZ3YKolVYKViseIZsQbhuJxYJZ4HT5UYyuJhUy1ETMEldVGS+raoveGBfmZ7KHJOg2h0lLr5S955VQpLlhzSwguXimOU2Wfb8i60ZcUrc861ZsUrsxOV2YlYICtRmX22NTuxNTtRmZWgNMQYwvHKHCzeEIPln8ycaChiODWmJmBJGOV4cWNmoxZMYIIyBy8ej0nLmcyGJRlqkjWpBY8pvKLO/VMlOaMURsh5YTJhtOJOuo4iBBqrSYOIdrSOiU4peWHS/AuqD08HqOU/M7X9hj4q7BbHKHnhsuLrGjMZoIxydRCLr2m5YdLbog4ovTOvSnVrxjPjlXy27H+vh+YlCEhhlwZRGCXPimsDYmenKfn4ckjAllWI2p1WA+sUuy6IopPyqowF2BGa9buIo3j3BOp5jIuCKIiQ1eR1mfXZQo1M4yjKBPD2i64IojhG6Tp9oenvKo2jeF7eYxoDT9jlQCwTtItjlPA4AGhNxlFemAz9BlSnlcpcDMQJlBcmtdI0CzJbGV8rPq2AsGmuBWJWQlthEmWW1kjChRcmbW6EbkHHtUDkhkrRCZL8Sxmxt1K0GfBdnLgQiOWC9nQO6KvDyMjIsrIyOyBds2aNRkNK5z0xZrhStKNOZBZxIRBFHGVNAeglm4CAADtAbG9vRxCEJBBRFE2Llj8o6CaTK5tluxCIvHBZfwdZA/Pz588PHDjg7u4eGBgYExPT3Y25GZn8YzKZKIoODQ0JBIJdu3bh2ZKSkkZHR3GPeXp65ufn79+/H0GQ2trayXJIWFiYzS61okDRVU3O2VYrMoLL4iogyv4e4bNkJNn106dPCIKIRCKdTldXV7d169YjR46gKPrlyxcEQYgeUSQSMRiMmpqahoaGR48e+fn5Xb9+Ha+Sj4/P5s2bL1269OrVq/Hx8WfPnpHaIz4p7UnjwDV3dhUQn5X1pEbLSQKxoKDA3d392zfjAl1/f39DQ8NsEEdHRxWKKfdfvXo1ODgYr5Kvr+/+/fuJ6pENYlPjEJ9N1s+SaIVNAVcB8X5eF3nrZ3iPeOzYsdzcXJVKRThgRo84MTGRl5e3c+dOBoOBD77e3t54Zl9f36SkJKIg2SB2tY6lhMO1nuoqIN7L6UjjkNUjoihaV1cXHx+PE3bo0KF3797N7hGvXr3KZDJLS0t1OuypVR6PZwoij8cDB6JmIiWc7hEJewMMPC3tST05NSySpLmrq6uiomL79u2rVq0aHx837RG/f//u4eHB5/MJ1efOnXMWiC1vh1NYJP4siTZaH3CVHrG5YVgQSZbpGxsb6+rqCKO/efMGQRCVSmUK4tjYGIIgN2/exLONjY35+fk5C8Tn5b3CKNJ/loRBrAm4CogoinJDpWMj360xiq15xGKxu7t7SUlJX1/fhw8f2Gy2r6/v+Pg4iqJ+fn5RUVENDQ3j4+NBQUG7du1Sq9X9/f0cDic6OprBYAwPD6Mo6uvrazo0q9VqBEEkEsmHDx9srYw1+YuvaTLi6OUba0xFQh5BpPxpKSkPQY2NjV2+fNnNzQ1BEDc3t9jYWGLKUlxc7OXltXLlysHBwebm5gMHDjAYDCaTWV5ertVqmUzm8uXLtVrtDBBRFI2NjWUwGAcOHCDBEqjopAK2B+FcqEcsvKzKOAPXeEQGZNbITA5pGRmA62kwFwKxVzeWHNJijZ8Wdp7KjHZBJFxT5n9HABcCEbvHelJeJtDOxdnExARzjj83N7eVK1eaTdyzZ89cAucfL5FIzCplMpn+/v5zJR0+fNiCaj5bVn+3z0IGpyS5FojqllHLD8Zq5/hrbm6eI0Xb2dlJnucGBwfn0iuXy+dK6uqa89mO+7mdQvi6Q5frEVEUzT+vyjm7wF8htfDD4IXL2mXYdB62P9fqEXHrC6Pk9dXQjU0AyJDEKstTIf0UiSuCiN1eY8kGOklZUwTAk30q8s6rYR4KXBREjMVwWXPDiH1OpVypzPi2Crg/y+S6IKIomhIuu8WdcxJNOdrmqnD6aWXeBfVcqZDEuzSIKIpmnFEK2LK3j6j/ETBzQJUL2rlh0ns5JM7rzam1J87VQcSe4CrvEUTI+BGyu5kd38bsMSJsZWR/DRdcUqWwZdmJbVR5a5EG0UjR4+Iu0SkFN0TKC5dJ4lqLrmkq0zvuZrbfTtMSx5107W3R1OntNO0dscmpSFuR3l6RbhpjLF6BJU3FV4qN8ZUZOkI4IdmYWdReYdBVkT5ZB1G7QT5WxCQSE3tXoqsQ6YquqCWxSj5bzmfJcs+1aVqML8TA9jsxWx8axJlmeVbWU3gF+/qqiKNMjVZgfp08BBFTYTxSEDUtRhAhNc0jYMlml+WFSwWEwEiTDJORKWwZP0IuMPzns+VYb21IErBlggi5IEKKRU5mxpNEHKU4Rpl7rq08VddEha8hzjS6q93im91+8DFHjx598eIFeL2Qa6R7RNAOOnjw4OvXr0FrhV4fDSJoF+3fv//t27egtUKvjwYRtIt+++239+/fg9YKvT4aRNAu2rFjx6dPn0BrhV4fDSJoFwUHB0ulcL1TDNoE5vTRIJqzCplxQUFBSiXoj5KR2SDHyKZBdIwdrZcSGBioVsN+59f65jgqJw2ioyxprRx/f/+ODlfZVsNao9AL2tZbylE5fXx8enpIeavVUTV0ihy6RwRtdi8vL70e6r13QFvEoI8GEbTZmUwm/nUH0Irh1keDCNo/7u7uX758Aa0Ven00iKBdtGzZsokJsr6gDLoxjtNHg+g4W1oh6fv370uWLLEio8tloUEE6vKvX78uX74cqEqKKKNBBOqoz58/e3h4AFVJEWU0iEAdNTAwsHr1aqAqKaKMBhGoo3p7e4mvxAJVDL0yGkSgLurs7PTz8wOqkiLKaBCBOkqr1QYEBABVSRFlNIhAHdXW1rZp0yagKimijAYRqKNkMtmWLVuAqqSIMhpEoI5qamravn07UJUUUUaDCNRRTU1NJ06cAKqSIspoEIE6SiqVEhtBAlUMvTIaRKAuUiqVQUFBQFVSRBkNIlBHqdXqwMBAoCopoowGEaijdDqdv78/UJUUUUaDCNRR3d3dPj4+QFVSRBkNIlBH9ff3e3l5AVVJEWU0iEAdNTQ0xGQygaqkiDIaRKCOGh0ddXd3B6qSIspoEIE6anx8nMFgAFVJEWU0iKAdRb+zYtbiNIhmzUJiJIPBoN/im21fGsTZNiE3hn6v2ax9aRDNmoXESPpLD2aNS4No1iwkRtLfvjFrXBpEs2YhMZL+GphZ49IgmjULiZH09xHNGpcG0axZSIwMDAzUaDQkKqCmaBpEQH77z3/+s2jy77///e/ixYsXLVp0+vRpQOqhV0ODCMhFO3fuXLRoEWLyt27duubmZkDqoVdDgwjIRTU1NR4eHiYcItHR0YB0U0ENDSI4L+3du5cAce3atfT+U6amp0E0tQa54adPn65evRpn8fjx4+Qqo5p0GkSgHjt48CCCIJ6envX19UAVQ6+MBnGWiybQJ6V9VZkd2N7ykzvP38kw7l2P7Tlv2FjeEMD2lr+TgW0sfztNi8UYtqmvzNAZ96sXabHt6EV4Nmzb+eyL/0TsvhF3+DaxBX3lZHFMlNgoCt/WnpCMn1ZmTG5ln4btWj/j9Haatjqn813t4Kz2UCOCBnGan25c1qSwZWn43vUsbHd6bCd5llwYKefjAXzLeixGwWcZk7D95FlyIZY0LTO+vXyKMVWRwpbyWXJsB/sIGbYRPctQKtKwKb0hjG1Qb5BgTIoy7FRvSOKz5KmRWH7iwKpkcspnydNPKVKjFMJIufTtyLRWUeGEBnHKS2VCrSRGNahHKX38VTskiFDIP1KMRRpEI4gl17Q5iZSnkPgJ8dnywa6p3xj8IRpEo49SWPJ25XfCkVQPFF3R5p1Twc8fUUMaRMwUH54OCqPkVIfPtP7PbvWnRSsIN8MfoEHEfFRf1c8PX1AgNtzXp4RTaXtyGkQMxLo73dxQmWmPQvXw63t6bigNIvwjwfQa0iBOt4cTzugeETM6DaIT0JuukgYRs8eLioU2NNfX6Hlhsum+hvqMBtEAYmUPb2FdI9bf1yeH0NeIUP/2zFSOHprNGAVsFN0jYvZ+UdWTEi6l+kzZtP70rBns78hB2hZej1h/T88No4dmB/EBTMzCA5HuEYHB40hFdZULbdZMg+hIPoDJelnZywtfUHdW6u/rufSsGRhAjlK08IZm7BqRvsXnKD6AyXlR2bPQ7jXX0JMVYPg4TtGLO128UHiXb7Iyb0RHx5iuzvwwTF8jOo4OgJIgH5pPRp+hQQSIg/NUvayyebLSrhm6fk2wY8cud3f3DQGB588ldXWM4h2Vvu9bXOyf3t5r/detT7rCu1/9DEGQVkXPoB7t6xm/dPF6UNDmn3766dChY/ernxF9m6enV052MY8rWrp06cqVK8PDIpXy7kE9unfv78Rr+e2aISK/5QDdIzqPpnloNlwj2jY087giBoNxu7zmSW1D5Z1Hvr5+ly5ex+EQpWUvWbLkdnmNWqUX8DOCt2xDEETV2jeoR2PPnGUwGLk5N9UqfWFBmZubW8XtB3gpHx/f9f4B/BSxVjOolHevXetzJiYRT/r119229oj19/rpyco8iHBSUTuG5q6O0Q9/K4hu6cL5q5s3B+Onv/yyJSw0gkjas2c/DmJ35xc3N7fr1wRE0ilOHFHKx8f3jwOHiSTOydhtW3fgp3aASPeITkJpfmrrKrt5YbatI/b3TmSI83Zs38lgMPChc80a70E92t87sWTJEnF6LoGUJKMAB7Hu2V8Igrx4/o5Iull0B0EQtUo/qEd9fHyvXOYSSefPJW0ICKRBnJ9jqVba0CPaNjRfOH+VuZKZn1cqbdYN6tGkKzwcRLVKjyBIQf4tAqnSkrs4iPfuPiWu9kwD799hr8v4+PgmXeERpWgQqQaRI+r70sbnEQf6v3t4eFy7yie4iY87h4PY2/0VQZDsrCIiSZyei4PY2PAJQZD8vNIntQ2mR0f7iMNBrK+mF7QdQQZgGS/udHPDbOgR+3rGEQTJzbmJ09bTNebr64eDOKhH/fzWxcedI0DEp72q1r6O9pEVK1bk55USSW3KXnw27XgQ79MgAobIEersmKxs2hS0Y8euT/9Tq9v6IyI4bFY0g8HQaYcH9WhC/HkPD4+HD14O9H8Xp+du3hyM94iDejQtNcvX16/u2V89XWMVtx9s3hwccoKFc2lhaI6MPLVx48+1j1/3dn8lILYcoCcrjuACuIwXlT22TlbeNjbv23uAwWAwmcwbheXNTVrmSuby5cubm7Sq1r5Dh44hCLIhIDDmdMKNwnIEQYglwIcPXh45csLNzW3NGu8Tx8PxmYrlHvH507ebNgUtXbpU3dZvmT8ilQYROESOUPiistuB76x0dYx+fK8kmEhLzWKuZBKnYAI0iI7gArgMDEQbl28s8JR8PXXFihWSjAJ1W3/ZrXtr1nibrstYKOjAJBpE4BA5QmGdjbPmHxKTEH9+29YdK1asCAzclMJL1/d9+2ERx2aor+7n0Y+BOYINoDLsuEZ0LDcOl0b3iEABcpSyFxU9yRA/BmYHpjSIjmIDqJyXVb0pC+tVARpEoAA5Spkd64h29FIgi9AgOooNoHIML0/ZcGcFJFL26aLfWQEKkKOULcAe8T79zoqj6AAo5yX2yRHbHgOzr6MCVooemgHi4zhVC+8tPnod0XF0AJRUd6ebZ8vTN8A6NrsV0T0iQHwcpwp7HnFhfcydBtFxdACUtAAnK/SXHgDy4zBVDff7hdELanuL1zV6YQS9z4rDCAEkSCcfw/YMo/gufKb1v5/dLY5RAjKfI9TQX4w1WjGdo3xY0GvqS0qHM84o72Z2OoIQQDJoEI2G1rSM8dkLZCmxKKldfJpK4zKKojSIRhA/f/588PfQFJas8LLm1Z3Bvx4PNDycdjQ+mnba8AA/1Tc8HGi0nNOQ2vhwoNFYZErOjEhCzrT4yVJ4auPDgdczajKp/a/Hww/y+yRnWvMuqAH1Y45TQ4NotGV2dvabN2/QMbTgoiotSsFjyZJDpMTBDZFyQ6ed4knckBYiz2SMlBdmNudUZPKkNFwsIRkPTJ5Ok8w1qYxR0WR9iCRBpCz9VKvgTP3z5887O6k0LtM9Itrb2xsTE+O4H/aPJR0/fryuru7H+ezKkZ6evmzZMiaTuX79+u3bt0dFRRUUFGA/MOj/XL1HPHToUFNTE0g3xcfHv3v3jiSNo6OjmzZtwj8jsWjRosWLFyMI4uXltWHDBpI0Okqsi4Ko0+nKysocZUSo5AgEAtPvmeDhxYsXQ1XJ2ZVxRRD1er2/v39HR8dscwCI0el0X758IU9Rd3f3hg0bTFlcsWIFeeocJdm1QGxvb9doND09PY4ynx1ydu3a9fHjRzsKWl/k4sWL+KCMIMjixYufPHlifVln5XQhEN+9e7d+/XpSeyNrvMhmsxUKchf51Gq1j48PTiGKoqGhoXw+35q6OTGPS4CIj8L19fVONDRg1RwOZ9GiRevWrcP1pqenHz16FHAdbFK38EEsKys7fvy4TUYhNbNGoxkfHydVBS7c29vbVMuLFy9Wr16tVkO61r2QQRwYGEBRND8/39QfTg8HBARotVqnVGNgYCAwMLC6utop2i0rXbAgisXigoICy413Survv//e19fnFNW40pMnTyYlJTmxAmZVL0AQv379qtVqU1JSzDaYjkRRNC8vb9++fVCZYqGBKBKJ2traxsbGoLKyaWXa2tq+fftmGuOU8Lt377CvOTY3O0X7bKULCsTi4mKBQDC7kVDF/PTTTyMjIzBU6evXr9u2bbt16xYMlVkgIBYVFaEo9gQDDDa1XIfg4GAws2bL1SBSExISEhMTiVNnBRYCiGFhYTdu3HCWBReA3tLS0u3btzv350FtEBsaGlAU1Wg0FKKhtbUVwto2NTUxGAzyHgv6YZOpCuLY2FhQUBAlnrQz9cHo6Ki7u7tpDFThvXv3OmvZlZIg9vb2arVaOLsWy2B9/vx5586dlvM4N/Xy5cscDgd8HSgGYnd398aNG527IAzeSYA13r17d9OmTYODgyD1UgzEkpISlUoF0kCO1TUxMQHt3V7Tlra1ta1aterVq1emkaSGqQGiXC4/fPgwqYYAI1yn0/n7+4PRNX8thw8fFovF85djjQRqgBgdHe2sB6qtMaL1eTo6Oqj1i+Lz+WFhYdY30O6cUIP48eNHZ03i7DbowitYW1vr4+ND9vup8ILY29u7e/duwJfMZGM0NjbmrGfA5tO07u5uPz+/9+/fz0eI5bLwgog/TWi59pRL1ev1+/fv//79O+Vq/u+Ng/3795N3gQQpiLm5uTU1NVT01g/r/PXr1/LychgewPlhVWdkWLJkyYwYB55CCmJSUlJeXp4D2wmbKJVKRa07ky0tLVu3biXPjJCCqNPpuru7yWs2DJI3bNgwPDwMQ02sqUNlZeXp06etyWlfHkhBtK8xlCv15s2boaEhSlQ7OTk5KyuLvKpCCmJpaWl5eTl5zYZHclNTE3nfZHJgM48dO0ZqPSEFMS0tTSgUOtCOMIs6duwY/P2in58fqUuJkILY1dVF3koBhFBqtVqY70Hr9XpPT09S7QYpiKS2GU7h79+/h/Y2UmNj44EDB0i1G6QgVldXFxYWktpyCIVfvnz58+fPEFassLDw4sWLpFYMUhDz8vIgfAmcVE/gwkdGRiB87DwhIYHsl/0gBbGnp6e9vR2A4yFU8c8//5w9exaqiu3evZvsT+lBCiJUbgBfmZKSEvBKLWh0d3cn+3N+kIJYW1ubmZlpwTSukATJSqpKpdq4cSPZBocUxPLy8vj4eLIbD7l8hUIRHh7u9Eo+fPiQzWaTXQ1IQezv74d5XY1srxDyP3z4QISdFUg1/JGtHVIQyW42teQ7dwGBzWY/fPiQbItBCmJ9fT38n30m2zeE/L6+vj/++IM4BRzYuHEjgDcnIQURzHUJYI/ORx3+ygTxeRofH5/169fPR6CVZb98+QJmdwxIQRwYGKDihxys9K7d2Y4fPz48POzv748giLu7O4CPEH/8+HH37t12V9j6gpCCaH0DXC3nTz/9RGwmBWAye+vWrYSEBABGhhTEjx8/Xr58GUD7qaViy5YtpltKbdiwQafTkdqES5cugfkUOaQgvn79+uDBg6SamHLCt23bZkohgiAMBoPsR0MOHDjQ2NgIwFaQgjg8PCyXywG0n0IqgoODAwMDly1bRuC4aNEisr/J7uXlpdfrAVgJUhABtJyKKt68eSMUCvfs2bN582YPD48gn+N7N8feL2n+8HLgwwvs+PjKEJg8xSIN4feGVDwPkXna6cuBD3VGIUT8y/uafVtijae4hJdGgaaR718OfCTkm6p+MfDPq8FujVUf1ocLxH379gUHB2/ZsmX9+vUrV64MDAwMCAjw8vKiIjSk1jkjTs4La0lhtSSHYBvdcw3/k0OkvDAptr99KBZpiJdyDRvdcydj8Ei8FJ4nOaSFi0mYKnXdKHNapCHzZE5MuFEFrg7XMinQUAdDBfhsWQpLnnZS8aqy37JB4ALxypUrixYtIoYePODr62u5Da6WmhopL+W3D3ahg3pqHC+r9CKOou0fS8/8wgViZ2dnQEDADBBjYmJcDTUL7U2Nlj+9qacKgqb1FETIG+7NebkJF4goigqFQlMQ165dC8+mNBb4AJNUldkpjmk19S6FwrUlvaKTc05AoQMR37iQYDEiIgKMjymhRRLfVi7ooBB8M6rKZ8vG5vieAHQgoiianp7OYDAQBPH29obhOSh4GE3jyCslXTO8S6FTXrhMKzN/pQgjiCMjI0FBQQiChIaGwgMBDDXhs2XlqVoKkTejqskhUnUTCSCOfUYfFXXduKIRnVKknVQIoxR8tpw4hJEyIsxnywURcn6ETBBpkifSEGlSRDAZww1vuXbiE48l5UfIBRHT5GCiIuX8SEzRNGlsuTBqSrup6rRouYijlMQoi66p6yp6YEDKvjrQIM60293MjtQoOTdUysdWidpEMW2ZidqMeI04Xk0ckgQtEcYDGQnqjISpDBkmmYkM4niVOH4qWwYWniYWyzkpx1SaOF4tmZ3ToCLrz3bJ2fb0GJUwSmlY8WpJP6V4eot6RNIgToF4R9zBC8c6ttxLXY+rJqh4PCgbyzyr47HkKSzZy0oq4SiMkt1Oa58x3lHo1GFDszBKIYhQVGQNU5G/2XW+wevjhUlFp5RTvzO4Q1TvEbkhUlXLqFkbWztZUTV/Tg6R5l3unO1OqseIz6hSwmVmrQNbJNVBnG+PqO/6yg2VPqig5EBsze+kJHWQRwUWXRpE6buRFJbcGndSOk9V4Vf4WeRHyG677PINL1R6r2iM0pBZWfnC5B5h5Jz3oGAYpl23RxRGK7LO6ax05ALIlspR5p6Hd9NJAdYjUnjWfP2EVN1s+2Sl9mY3n73wB+UZvx9uiFSn/ApD/ze7Dny2tJzKyzfcULtmzbwwaTG/b4afFvxpRrwmjaOYDQEMMa44ND8v7+GFSaHF7uqF6n+f0Kko7iajhskhLQM932Agb0YdBJGy22mg7zV7enrxuCKHLJvbs3yTfkopSdCQ4WaHyCQVRGGU8uZ1zQwIYDjlR0htvbMSHhZZWFA2H4ycDCI3VFpVMOoQaMgQQiqIuUndqdEwjs52DM3r/QMoDOKn+iFSx2WJ8O224MMe7qt8124KPXblbukADmtiTAGT6V2YJVvntxlBkPXrgi8mlhEchx+/ymR6r/UOjAznXzl3h7yh+WEZtoAPQxc4ow62gkg8X8xkMvFO8W5l7dbg7W5ubr6+fkePhkibdURnOVcS0SMO9H/PEOcFb9m2YsWKbVt3XLnM7e+dIIpbE+CG2jhrrpLo+BEKggDHBopzlMuXu//y877CLFlBZsvWLQc3rN/+oOLL46qJc/E3ly1bvnXLwczUvx5Vjp+MSF2yZElpPnaFcDb2htty9+sX71fe7LuYWOa7dhN5ID6umkhhyZobodsoD1vQFtmwfNPd+QVBEKJHfPTw1dKlSzPEeeq2/pd1f2/eHHzkyAkcIAtJBIgScb6np1fxjQpVa19+Xqmnp5eAn2ENf0Qem68Ri65o0jitjuWPkBbNEni4r7pT3IPHlOSpEQS5dvEeDiKCIGJBI55Umq9BEIR75dHjqgk/n6A/9sUQQo4ePEsqiIJIxbNbvTM6JKefYpMVW+6szADx1193h4dFElg8flSPIMjr+n8G9aiFJALEkBOsQ4eOEcU//U/9trGZOLUmYDOIeRdVqZw2wuuODWzbcmhb8GFTmWu9A08cuUCAWHPbeG1afWsQQZCLZ8sfVY4vXbosJlpClDofX0IqiPwI+b1ccj8rYwfWtg7NM0BcsWKFKC2bIKZdM4QgSHZW0aAetZBEgJguylm2bNkpTlzRjdtqlT1vEtoMYv7FViFpPeLGDTuJaxcisGdXBAEiPkw/rpogQKwq1SMIcjb2BgFi0vkqUkEURMhrcjvtYIXUIvwIWbktd1ZMQTTFDmdR3/cNQRChQGIhaVCPEiDq+77l55UeOXIC/+xOZOQphcy2F2hsBrH4miaVNBB37gj55ed9Il696ZEvabIAIt4jxnKyCBDPxd8kFUQ+Ww7h0DyfHrGvZxxBkLTULKJH1GqwAaf4RoWFJFMQiYLv38ncLDyfAAAFrklEQVQzMwpWrVp99GgIEWlNwOY7K/dzOgWRZE1WIsNS1nj5P7zzlaCqILOFmKwgCDK7R3xcNbHWO/C3nSyiyK4doaSCyAuXSd+NkNq92SGcH2HbLT7THnFQj27f9uuJ4+EEMffvPUcQ5E1jk+UkokcsvlHx918yojg3Oc3X1484tSZgc48o/3skJVxGeN2xgcqbfZ6r/U4cuXCvbPhGljz8xLUlS5bkpH+00CM+rppgh3IRBLl6ofpx1cTp6Awm05s8EO8VjfFCYXxU1nBnxYZZ86Ae9fX1Y4VHPalt6OsZL75RgSCIRJyvUQ88qW3YuPHn3/cfxAGykESAePCPI4GBm6qrnqhV+vvVz7Zs2coKj7KGPyKPzSCiKMoLk5VnjzgWQUJaVUk/OyTZzxd7Z3RT4K4LibfwJHzANdsj3isbPnwgHufvl5/3XTlfiSDI7SJS3pvJudCRGgXjgnYKW1ZmyzXioB7NyS729PRauXKlVjM40P+dnyL29fX7d7K8bp1/zOkEdVs/TomFJAJEabPuxPHwf3//hlfO116/JmjXDBGQWROwB8TM+FZxnJpAx6UCwkhlmQC6KTOKorZeI1oDB8g89oD412M9j7TRGXKssdsq3+24hCO9CNVBxCYrzba/YM9nyfKT5hz7bhVo3d2ZZo81nv5m493dmUEb9ziWwrkUubszly93N5u6Y9sxC3VIP6POimsjnSm7FGAgUvl5RHt6RBRFGx8OpLDmnLI8qPhSkqcyexRIWszGl+SpbhW0W4DAjqS5FJXkqQoypWZTyws7LCjihkq/WfWNU7tQml8hwxPaoB8Dc+DYbSeIKIpK4tok8fA+DGaBJ/uSUk8qy/ja+dFCYmmsRxTaNmt2IEbzF2U/iNgFMkt2Szxon1+pVSr3QgfkL9tT/RrxeohUY8c1ovGnPYFyw+YcoKmFmoXa5l/p5kdA/QofiqICiveI2GNg8/nSw8QEyg2VloiGLDiS0kk55zsgf5EU7xQMPaKrDs3EJU8KS7YgrxeF0UphFOx9oQmILjlZISjEA5K41hSWrOAqKa8sge9TM//UcUOlBRfUM5oJ7akgQlZuy4Ox859eOFbCvCYrM7zyqrpPGCVPYcsz4jUVeZ/B0zN/jaUivYjTxg2Vpp9Wapq+zGggzKdUn6wYXhWwfUHbgksaavoz41q5YVJumJQfoUg92SqMVqaeahWenDpSOcq5T5XTU405MTknJ8OcVsFJZSqn1RCJiU092ZrKMcRwjFrwpNRTbbiiVA4WSDOkGv8b6pN2Wik6rRZEKnnhMm6olBcmy0psU34wbxELrXZ6Ep9t81t8ju3S5inNkT3iDGdI3w5VZ3YUX9NI4pSSWGVGzNSRFYeFJWeMMZnxU0kZZ5SSOAWRWRIrN4bPTOXHpJ1RZMYqJWcUGbjkMwpJrDLTcGTEYsVxjZlxSuOpQaYkXiGZlI8lxSizE1oLL2uKrmpq8jo1UvOfvJjRLjhP+bY/9DBPdBxbnEQQ4XTYQq0V1YdmGsQFQqaIo6zMoPI+KyyZusX8/VNrvxi7QDxJ8WZkJrSW8ii8jpjCkqMT5n1Ag2jeLnDG1t7sFZ1SOva6DZi06uzu9NOtcxmWBnEuy0Aan35KWcbvBEaPAxWlRsoV7+dcqaBBhBQ4C9XKSmjLPqv+UDfqQErIE9Xe+v1OemcKSzasn2NUNjSVBtGCx+FNKrqqEUYqUsJlpnt14xt+m8ZYEyY2Hbcmc/KJqU3BrcnPC5fxWTJJbKv2R6tmNIjw0vbjmo2hn/UTxDEy8O3zwDfsFP9vSMIiiTwDE5/xg4jRT3weNEiYkQ0TMk3OpGRDPF58WpFvM/PjeebYi3R202gQZ9uEjnGCBWgQnWB0WuVsC9AgzrYJHeMEC9AgOsHotMrZFqBBnG0TOsYJFqBBdILRaZWzLUCDONsmdIwTLPD/NqnBqPWzfqgAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "3c364c80",
   "metadata": {},
   "source": [
    "### Langchain Agent in V1\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "```\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "now ->\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750be741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using Ollama qwen3 + nomic-embed-text\n",
      "✓ Connected to existing Chroma vector store\n"
     ]
    }
   ],
   "source": [
    "# use the langchain pre-release for latest features\n",
    "# pip install --pre -U langchain langchain-ollama, langchain-chroma, langchain-core langchain-community\n",
    "\n",
    "# Required: pip install langchain langchain-ollama, langchain-chroma, langchain-core langchain-community\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# LLM & VECTOR STORE SETUP\n",
    "# ============================================================================\n",
    "# Ollama LLM with Qwen3\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3\", \n",
    "    base_url=\"http://localhost:11434\", \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Ollama Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Chroma Vector Store (assumes data already exists from previous code)\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"..\\\\01 Semantic Search\\\\chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"✓ Using Ollama qwen3 + nomic-embed-text\")\n",
    "print(\"✓ Connected to existing Chroma vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53deb9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing Vector Store Connection...\n",
      "✓ Collection 'pdf_collection' found with 593 documents\n",
      "\n",
      "✓ Sample search for 'methodology':\n",
      "  1. Page 59: involves providing accurate information and transparency about the model’s uncertainty and\n",
      "limitatio...\n",
      "  2. Page 60: This method could lead to biased or incomplete feedback due to the diversity of opinions\n",
      "among label...\n",
      "  3. Page 86: the input (e.g., the sentiment in sentiment analysis; category in classification tasks; etc.).\n",
      "78Ret...\n",
      "\n",
      "✓ Sample metadata: {'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'subject': '', 'total_pages': 174, 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'page': 59, 'start_index': 3525, 'title': '', 'trapped': '/False', 'page_label': '60', 'author': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creationdate': '2025-02-11T01:48:37+00:00'}\n",
      "✓ Vector store verification complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VECTOR STORE VERIFICATION\n",
    "# ============================================================================\n",
    "print(\"\\n🔍 Testing Vector Store Connection...\")\n",
    "\n",
    "# Test 1: Check collection info\n",
    "collection = vector_store._collection\n",
    "doc_count = collection.count()\n",
    "print(f\"✓ Collection '{collection.name}' found with {doc_count} documents\")\n",
    "\n",
    "\n",
    "# Test 2: Sample similarity search\n",
    "test_query = \"methodology\"\n",
    "results = vector_store.similarity_search(test_query, k=3)\n",
    "print(f\"\\n✓ Sample search for '{test_query}':\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. Page {doc.metadata.get('page', '?')}: {doc.page_content[:100]}...\")\n",
    "\n",
    "\n",
    "# Test 3: Check available metadata\n",
    "if results:\n",
    "    sample_metadata = results[0].metadata\n",
    "    print(f\"\\n✓ Sample metadata: {sample_metadata}\")\n",
    "\n",
    "print(\"✓ Vector store verification complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82f15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Agentic RAG ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RETRIEVAL TOOL\n",
    "# ============================================================================\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve relevant information from the document to answer the query.\"\"\"\n",
    "    print(f\"🔍 Searching: '{query}'\")\n",
    "    \n",
    "    # Perform similarity search\n",
    "    docs = vector_store.similarity_search(query, k=4)\n",
    "    \n",
    "    # Format for LLM\n",
    "    content = \"\\n\\n\".join(\n",
    "        f\"Page {doc.metadata.get('page', '?')}: {doc.page_content}\" \n",
    "        for doc in docs\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Found {len(docs)} relevant chunks\")\n",
    "    return content, docs\n",
    "\n",
    "# ============================================================================\n",
    "# AGENT CREATION\n",
    "# ============================================================================\n",
    "# Tools for the agent (single tool only)\n",
    "tools = [retrieve_context]\n",
    "\n",
    "# Agent prompt - simplified for single tool\n",
    "prompt = \"\"\"You are a research assistant with a document retrieval tool.\n",
    "\n",
    "Tool:\n",
    "- retrieve_context: Search the document for relevant information\n",
    "\n",
    "Always use the tool to find relevant information before answering.\n",
    "Cite page numbers and be thorough.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717c7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agentic RAG\n",
    "rag_agent = create_agent(llm, tools, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4810cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching: 'main methods'\n",
      "✓ Found 4 relevant chunks\n"
     ]
    }
   ],
   "source": [
    "rag_agent\n",
    "\n",
    "response = rag_agent.invoke({'messages': \"What are the main methods mentioned in this paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90bd0b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are the main methods mentioned in this paper?', additional_kwargs={}, response_metadata={}, id='196667ab-5349-43a7-87fe-b29e72f758c2'),\n",
       "  AIMessage(content='<think>\\nOkay, the user is asking about the main methods mentioned in the paper. Since I need to use the retrieve_context tool, I should start by calling that function with the query specifically asking for the main methods. The tool will search the document and return relevant information. I have to make sure to cite page numbers and be thorough in the answer. Let me structure the tool call correctly.\\n</think>\\n\\n', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T11:35:04.4331886Z', 'done': True, 'done_reason': 'stop', 'total_duration': 864400800, 'load_duration': 48579100, 'prompt_eval_count': 194, 'prompt_eval_duration': 164461900, 'eval_count': 110, 'eval_duration': 650697800, 'model_name': 'qwen3'}, id='run--a4bb414a-b7dc-438c-899d-6735f18f1402-0', tool_calls=[{'type': 'tool_call', 'id': '37df6881-bafd-4b9d-8167-29a5f41458af', 'name': 'retrieve_context', 'args': {'query': 'What are the main methods mentioned in this paper?'}}], usage_metadata={'input_tokens': 194, 'output_tokens': 110, 'total_tokens': 304}),\n",
       "  ToolMessage(content='Page 103: Method Non-football (DROP) Football (DROP) GSM8k\\nZero-Shot 43.86 51.77 16.38\\nStandard Prompting 58.78 62.73 17.06\\nChain-of-Thought 74.77 59.56 60.87\\nLeast-to-Most 82.45 73.42 62.39\\nTable 36: Accuracies of different prompting methods on GSM8k and DROP benchmarks. Source:\\nZhou et al. [244]\\nDECOMP is a text-based prompting strategy that decomposes complex tasks into simpler\\nsubtasks and generates a plan to solve the task, similar to Least-to-Most prompting. The\\ncore idea of Decomposed Prompting involves dividing a complex task into multiple simpler\\nsubtasks. Each subtask is addressed separately using LLMs, and their results are then combined\\nto produce the final outcome. Tasks are decomposed based on their inherent structure. For\\ninstance, a question-answering task might be split into subtasks involving information retrieval,\\ncomprehension, and synthesis. The model can process each step more effectively by focusing\\non these individual components.\\nFigure 48: The DECOMP framework. Source: Khot et al. [175]\\n\\nPage 4: the transformative impact of LLMs across various domains, including healthcare, finance,\\neducation, law, and scientific research.\\n• Section 3 focuses on the fundamental building blocks of LLMs, covering data preprocess-\\ning techniques, pre-training methodologies, and model adaptation strategies. It explores\\nvarious pre-training approaches, including unsupervised, supervised, and semi-supervised\\nlearning, emphasizing their impact on model performance and adaptability. The section\\nalso examines different data sources used in LLM training, categorizing them into gen-\\neral data like Web pages, books, and conversation text, specialized data such as scientific\\nliterature and code, and widely used datasets like Wikipedia, BookCorpus, and Com-\\nmonCrawl. It details the critical data preprocessing steps, such as quality filtering, data\\ncleaning, deduplication, and tokenization, and their role in preparing data for effective\\nLLM training. Moreover, it discusses model adaptation techniques like instruction tuning\\n\\nPage 95: Taylor need to do before this? A. get a certificate , B. teach\\nsmall children, C. work in a school\\nARC Science Choose your answer to the question: Which technology was\\ndeveloped most recently? A. cellular telephone, B. television,\\nC. refrigerator, D. airplane\\nQASC Science Choose your answer to the question: What is described in terms\\nof temperature and water in the air? A. storms; B. climate;\\nC. mass; D. seasonal; E. winter; F. density; G. length\\nHellaSWAG Event Choose your answer to the question: We see a chair with a pillow\\non it. A. a man holding a cat does curling. B. a man holding a\\ncat starts hitting objects on an item. C. a man holding a cat is\\nwrapping a box. D. a man holding a cat sits down on the\\nchair.\\nNumerSense Numerical a square is a shape with 〈mask〉equally length sides. (four)\\nProtoQA Prototypical Use simple words separated by commas to name something in\\nyour life that could cause you to lose weight. ( Eating less,\\nexercising more, stress.)\\n\\nPage 83: Figure 36: Taxonomy of in-context learning. The training and the inference stage are two main\\nstages for ICL. During the training stage, existing ICL studies mainly take a pre-trained LLM as the\\nbackbone and optionally warm up the model to strengthen and generalize the ICL ability. Towards\\nthe inference stage, the demonstration design and the scoring function selection are crucial for the\\nultimate performance. Source: Dong et al. [265]\\nDesigning branch). The selection aims to choose good examples for ICL using unsupervised 72\\nor supervised methods. For example, KATE [186] and EPR [208] select demonstrations based\\non similarity. Ordering the selected demonstrations is also an important aspect of demonstra-\\ntion design. Lu et al. [190] have proven that order sensitivity is a common problem and affects\\nvarious models. To address this problem, studies have proposed several training-free meth-\\nods for ordering demonstrations. Liu et al. [186] sorted examples based on similarity, while', name='retrieve_context', id='cbf147f2-abf8-44ff-844b-7ace4f8ed374', tool_call_id='37df6881-bafd-4b9d-8167-29a5f41458af', artifact=[Document(id='600b2c00-838b-47a4-9f3d-12e54d5c1373', metadata={'author': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'total_pages': 174, 'subject': '', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'moddate': '2025-02-11T01:48:37+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'page_label': '104', 'title': '', 'creationdate': '2025-02-11T01:48:37+00:00', 'page': 103, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'start_index': 0}, page_content='Method Non-football (DROP) Football (DROP) GSM8k\\nZero-Shot 43.86 51.77 16.38\\nStandard Prompting 58.78 62.73 17.06\\nChain-of-Thought 74.77 59.56 60.87\\nLeast-to-Most 82.45 73.42 62.39\\nTable 36: Accuracies of different prompting methods on GSM8k and DROP benchmarks. Source:\\nZhou et al. [244]\\nDECOMP is a text-based prompting strategy that decomposes complex tasks into simpler\\nsubtasks and generates a plan to solve the task, similar to Least-to-Most prompting. The\\ncore idea of Decomposed Prompting involves dividing a complex task into multiple simpler\\nsubtasks. Each subtask is addressed separately using LLMs, and their results are then combined\\nto produce the final outcome. Tasks are decomposed based on their inherent structure. For\\ninstance, a question-answering task might be split into subtasks involving information retrieval,\\ncomprehension, and synthesis. The model can process each step more effectively by focusing\\non these individual components.\\nFigure 48: The DECOMP framework. Source: Khot et al. [175]'), Document(id='02323f2f-d907-4229-ba40-9e7136f65dc3', metadata={'moddate': '2025-02-11T01:48:37+00:00', 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'creationdate': '2025-02-11T01:48:37+00:00', 'producer': 'pdfTeX-1.40.25', 'page_label': '5', 'subject': '', 'page': 4, 'total_pages': 174, 'title': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'creator': 'LaTeX with hyperref', 'author': '', 'start_index': 899}, page_content='the transformative impact of LLMs across various domains, including healthcare, finance,\\neducation, law, and scientific research.\\n• Section 3 focuses on the fundamental building blocks of LLMs, covering data preprocess-\\ning techniques, pre-training methodologies, and model adaptation strategies. It explores\\nvarious pre-training approaches, including unsupervised, supervised, and semi-supervised\\nlearning, emphasizing their impact on model performance and adaptability. The section\\nalso examines different data sources used in LLM training, categorizing them into gen-\\neral data like Web pages, books, and conversation text, specialized data such as scientific\\nliterature and code, and widely used datasets like Wikipedia, BookCorpus, and Com-\\nmonCrawl. It details the critical data preprocessing steps, such as quality filtering, data\\ncleaning, deduplication, and tokenization, and their role in preparing data for effective\\nLLM training. Moreover, it discusses model adaptation techniques like instruction tuning'), Document(id='12665abf-e4df-4e93-9090-37a5b6882bbe', metadata={'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'start_index': 894, 'page_label': '96', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'total_pages': 174, 'trapped': '/False', 'moddate': '2025-02-11T01:48:37+00:00', 'page': 95, 'keywords': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'author': ''}, page_content='Taylor need to do before this? A. get a certificate , B. teach\\nsmall children, C. work in a school\\nARC Science Choose your answer to the question: Which technology was\\ndeveloped most recently? A. cellular telephone, B. television,\\nC. refrigerator, D. airplane\\nQASC Science Choose your answer to the question: What is described in terms\\nof temperature and water in the air? A. storms; B. climate;\\nC. mass; D. seasonal; E. winter; F. density; G. length\\nHellaSWAG Event Choose your answer to the question: We see a chair with a pillow\\non it. A. a man holding a cat does curling. B. a man holding a\\ncat starts hitting objects on an item. C. a man holding a cat is\\nwrapping a box. D. a man holding a cat sits down on the\\nchair.\\nNumerSense Numerical a square is a shape with 〈mask〉equally length sides. (four)\\nProtoQA Prototypical Use simple words separated by commas to name something in\\nyour life that could cause you to lose weight. ( Eating less,\\nexercising more, stress.)'), Document(id='1808df2f-f429-4ccd-9284-bb494e751617', metadata={'start_index': 1025, 'page_label': '84', 'creator': 'LaTeX with hyperref', 'keywords': '', 'page': 83, 'title': '', 'creationdate': '2025-02-11T01:48:37+00:00', 'producer': 'pdfTeX-1.40.25', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'subject': '', 'moddate': '2025-02-11T01:48:37+00:00', 'total_pages': 174}, page_content='Figure 36: Taxonomy of in-context learning. The training and the inference stage are two main\\nstages for ICL. During the training stage, existing ICL studies mainly take a pre-trained LLM as the\\nbackbone and optionally warm up the model to strengthen and generalize the ICL ability. Towards\\nthe inference stage, the demonstration design and the scoring function selection are crucial for the\\nultimate performance. Source: Dong et al. [265]\\nDesigning branch). The selection aims to choose good examples for ICL using unsupervised 72\\nor supervised methods. For example, KATE [186] and EPR [208] select demonstrations based\\non similarity. Ordering the selected demonstrations is also an important aspect of demonstra-\\ntion design. Lu et al. [190] have proven that order sensitivity is a common problem and affects\\nvarious models. To address this problem, studies have proposed several training-free meth-\\nods for ordering demonstrations. Liu et al. [186] sorted examples based on similarity, while')]),\n",
       "  AIMessage(content='<think>\\nOkay, let\\'s tackle the user\\'s question about the main methods mentioned in the paper. First, I need to go through the provided tool response and extract the relevant information.\\n\\nLooking at the tool response, there are several pages mentioned. Page 103 talks about different prompting methods like Zero-Shot, Standard Prompting, Chain-of-Thought, and Least-to-Most, with their accuracies on specific benchmarks. That\\'s a list of methods, so those are definitely main methods.\\n\\nThen, on Page 4, there\\'s a mention of DECOMP as a text-based prompting strategy that decomposes tasks into subtasks. The core idea is breaking down complex tasks into simpler ones and using LLMs to handle each part. That\\'s another method, DECOMP.\\n\\nPage 95 has some example questions and answer choices, but it\\'s more about tasks rather than methods. However, the mention of NumerSense, ProtoQA, and HellaSWAG might be specific tasks or datasets, not methods. But the user is asking about methods, so maybe those are part of the methods used in the paper? Wait, the user is asking about the paper\\'s methods, so if these are part of the paper\\'s content, they might be methods. But the tool response doesn\\'t explicitly say they are methods; it\\'s more about examples. So maybe not.\\n\\nPage 83 discusses in-context learning (ICL) with stages: training and inference. During training, they use pre-trained LLMs and warm up the model. For inference, demonstration design and scoring functions are crucial. Also, methods like KATE and EPR for selecting demonstrations based on similarity. Ordering demonstrations is another aspect, with methods like Liu et al. sorting examples by similarity. So these are methods related to ICL.\\n\\nPutting it all together, the main methods mentioned are:\\n\\n1. Prompting methods: Zero-Shot, Standard Prompting, Chain-of-Thought, Least-to-Most (from Page 103).\\n2. DECOMP (from Page 4).\\n3. In-context learning (ICL) methods, including demonstration selection (KATE, EPR) and ordering (Liu et al.) (from Page 83).\\n\\nI need to make sure to cite the page numbers for each method. Also, check if there are any other methods mentioned. The tool response also mentions model adaptation techniques like instruction tuning on Page 4, but that\\'s part of the section on LLM building blocks, not a specific method. So the main methods are the ones listed above.\\n</think>\\n\\nThe paper mentions several key methods, primarily focusing on prompting strategies, task decomposition, and in-context learning (ICL). Here\\'s a structured summary:\\n\\n1. **Prompting Methods** (Page 103):  \\n   - **Zero-Shot Prompting**: Achieved 43.86% accuracy on GSM8k.  \\n   - **Standard Prompting**: 58.78% accuracy.  \\n   - **Chain-of-Thought (CoT)**: 74.77% accuracy.  \\n   - **Least-to-Most Prompting**: 82.45% accuracy.  \\n   These methods vary in how they structure prompts to guide language models (LLMs) for problem-solving.\\n\\n2. **DECOMP Framework** (Page 4):  \\n   A text-based strategy that decomposes complex tasks into simpler subtasks (e.g., information retrieval, comprehension, synthesis). Results are combined to produce the final output. This approach resembles \"Least-to-Most\" prompting but emphasizes structured task breakdown.\\n\\n3. **In-Context Learning (ICL)** (Page 83):  \\n   - **Training Stage**: Uses pre-trained LLMs, optionally warmed up to enhance ICL capabilities.  \\n   - **Inference Stage**: Focuses on demonstration design (e.g., selecting examples via unsupervised/supervised methods like **KATE** [186] and **EPR** [208], which prioritize similarity) and scoring function selection.  \\n   - **Demonstration Ordering**: Addressed via methods like **Liu et al.** [186], which sort examples by similarity to mitigate order sensitivity issues.  \\n\\nThese methods highlight the paper\\'s exploration of prompting, task decomposition, and ICL techniques to improve LLM performance across diverse tasks.', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T11:35:12.1467563Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5608516700, 'load_duration': 47867300, 'prompt_eval_count': 1273, 'prompt_eval_duration': 121202900, 'eval_count': 898, 'eval_duration': 5432524900, 'model_name': 'qwen3'}, id='run--742d6633-7eb9-4a4a-9b66-340c6536a4c4-0', usage_metadata={'input_tokens': 1273, 'output_tokens': 898, 'total_tokens': 2171})]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUERY FUNCTION\n",
    "# ============================================================================\n",
    "def ask(question: str):\n",
    "    \"\"\"Ask the agentic RAG a question.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for event in rag_agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        msg = event[\"messages\"][-1]\n",
    "        \n",
    "        # Show tool usage\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"\\n🔧 Using: {tc['name']} with {tc['args']}\")\n",
    "        \n",
    "        # Show final answer\n",
    "        elif hasattr(msg, 'content') and msg.content:\n",
    "            print(f\"\\n💬 Answer:\\n{msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed913f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: First find the methodology, then look for evaluation results.\n",
      "============================================================\n",
      "\n",
      "💬 Answer:\n",
      "First find the methodology, then look for evaluation results.\n",
      "\n",
      "🔧 Using: retrieve_context with {'query': 'methodology'}\n",
      "\n",
      "🔧 Using: retrieve_context with {'query': 'evaluation results'}\n",
      "🔍 Searching: 'methodology'\n",
      "🔍 Searching: 'evaluation results'\n",
      "✓ Found 4 relevant chunks\n",
      "✓ Found 4 relevant chunks\n",
      "\n",
      "💬 Answer:\n",
      "Page 60: a crucial role in providing feedback on the model’s outputs. Ensuring that labellers have\n",
      "adequate qualifications is vital; despite stringent selection criteria, mismatches in intentions\n",
      "between researchers and labellers can still occur, potentially compromising feedback quality\n",
      "and LLM performance [106]. To address this, the InstructGPT initiative includes a screening\n",
      "process to select labellers whose evaluations closely align with those of researchers [205]. In\n",
      "some studies, using “super raters” ensures the highest quality of feedback by selecting the most\n",
      "consistent labellers for critical tasks [165].\n",
      "Three primary methods are used to collect human feedback and preference data:\n",
      "• Ranking-based approach. Human labellers evaluate model outputs in a coarse-\n",
      "grained fashion, often choosing only the best output without considering finer details.\n",
      "This method could lead to biased or incomplete feedback due to the diversity of opinions\n",
      "\n",
      "Page 60: This method could lead to biased or incomplete feedback due to the diversity of opinions\n",
      "among labellers and the neglect of unselected samples. To improve this, later studies\n",
      "introduced the Elo rating system to establish a preference ranking by comparing outputs,\n",
      "thereby providing a more nuanced training signal [165, 85].\n",
      "• Question-based approach. This method involves labellers providing detailed feed-\n",
      "back by answering specific questions designed to assess alignment criteria and additional\n",
      "constraints. For example, in the WebGPT project, labellers evaluate the usefulness of\n",
      "retrieved documents to answer given inputs, helping to filter and utilize relevant informa-\n",
      "tion [124].\n",
      "• Rule-based approach. This approach involves the use of predefined rules to generate\n",
      "detailed feedback. For instance, Sparrow uses rules to test whether responses are helpful,\n",
      "correct, and harmless. Feedback is generated both by comparing responses and assessing\n",
      "\n",
      "Page 27: Flan-PaLM (540 B) 67.6\n",
      "PubMedGPT (2.7 B) 50.3\n",
      "DRAGON (360 M) 47.5\n",
      "BioLinkBERT (340 M) 45.1\n",
      "Galactica (120 B) 44.4\n",
      "PubMedBERT (100 M) 38.1\n",
      "GPT-Neo (2.7 B) 33.3\n",
      "Table 7: Performance comparison of different models on the MedQA (USMLE) benchmark. Source:\n",
      "Singhal et al. [213].\n",
      "Despite these remarkable results, human evaluation reveals key gaps in Flan-PaLM responses\n",
      "and remains inferior to clinicians [213]. To resolve this issue, researchers introduced “instruction\n",
      "tuning”35 to align the Flan-PaLM model to the medical domain. Thus, Instruction tuning\n",
      "can be seen as a lightweight way (data-efficient, parameter-efficient, compute-efficient during\n",
      "training and inference) of training a model to follow instructions in one or more domains.\n",
      "Instruction tuning adapted LLMs to follow better the specific type of instructions used in the\n",
      "family of medical datasets. The result was Med-PaLM, a model that significantly reduces\n",
      "the gap (or even compares favourably) to clinicians on several evaluation axes, according to\n",
      "\n",
      "Page 30: style closed-source model.\n",
      "Regarding the evaluation tasks, the models are assessed on a range of financial NLP tasks,\n",
      "as shown below:\n",
      "• Sentiment Analysis (SA):This task involves analyzing the sentiment embedded within\n",
      "financial documents, such as market reports and news articles. The capability to accu-\n",
      "rately discern sentiment is crucial for applications such as market prediction and the\n",
      "formulation of trading strategies.\n",
      "• Named Entity Recognition (NER): Essential for extracting actionable insights from\n",
      "financial documents, this task focuses on the identification and categorization of salient fi-\n",
      "nancial entities, including but not limited to company names, stock tickers, and monetary\n",
      "values.\n",
      "• Question Answering (QA): FinLLMs are tasked with providing cogent answers to\n",
      "queries based on an expansive financial corpus. This benchmark often requires the syn-\n",
      "thesis of information from dense financial reports or news events.\n",
      "• Text Classification (TC): The classification of financial documents into predefined\n",
      "\n",
      "💬 Answer:\n",
      "<think>\n",
      "Okay, let me try to figure out how to approach this. The user asked to first find the methodology and then look for evaluation results. From the previous tool calls and responses, I see that the assistant is working with a document that includes various sections on methodologies and evaluation results.\n",
      "\n",
      "First, I need to identify the methodology sections. The user mentioned using the retrieve_context function, so I should look for parts of the document that discuss methods, approaches, or techniques used in the research. In the provided tool response, there are mentions of different methods like ranking-based, question-based, and rule-based approaches. These seem to be part of the methodology section, explaining how human feedback was collected.\n",
      "\n",
      "Next, the evaluation results would be the sections where the outcomes of the methods are discussed. The tool response includes a table comparing different models' performance on the MedQA benchmark, which is an evaluation result. There's also mention of specific tasks like Sentiment Analysis, Named Entity Recognition, etc., which are evaluation tasks. Additionally, the performance metrics like accuracy percentages for models such as Flan-PaLM and Med-PaLM are part of the evaluation results.\n",
      "\n",
      "I need to make sure that the methodology is clearly outlined first, detailing the approaches used to collect feedback, and then present the evaluation results, including the metrics and comparisons. The user also wants citations with page numbers, so I should reference the specific pages where each part is mentioned. For example, the ranking-based approach is on page 60, and the MedQA results are on page 27. \n",
      "\n",
      "I should structure the answer by first summarizing the methodology sections, then moving on to the evaluation results, ensuring each part is cited correctly. It's important to check if there are other sections that might be part of the methodology or evaluation, but based on the provided content, the main points are covered. Also, note any discrepancies or additional details that might be relevant, like the mention of instruction tuning improving model performance.\n",
      "</think>\n",
      "\n",
      "### Methodology  \n",
      "The document outlines three primary methods for collecting human feedback and preference data:  \n",
      "1. **Ranking-based approach**: Labellers evaluate model outputs in a coarse-grained manner, selecting the best output without detailed analysis. However, this method risks biased or incomplete feedback due to diverse opinions and neglect of unselected samples [165, 85].  \n",
      "2. **Question-based approach**: Labellers provide detailed feedback by answering specific questions to assess alignment criteria. For example, in the WebGPT project, labellers evaluated the usefulness of retrieved documents for answering inputs [124].  \n",
      "3. **Rule-based approach**: Predefined rules generate feedback. Sparrow uses rules to test if responses are helpful, correct, and harmless, combining response comparisons with assessments [165].  \n",
      "\n",
      "Additionally, **instruction tuning** is highlighted as a lightweight method to align models with domain-specific instructions, exemplified by adapting Flan-PaLM to create Med-PaLM for medical tasks [213].  \n",
      "\n",
      "---\n",
      "\n",
      "### Evaluation Results  \n",
      "Key evaluation findings include:  \n",
      "1. **Model Performance on MedQA Benchmark**:  \n",
      "   - Flan-PaLM (540 B): 67.6  \n",
      "   - PubMedGPT (2.7 B): 50.3  \n",
      "   - Med-PaLM (instruction-tuned): Outperforms Flan-PaLM and approaches clinicians on multiple axes [213].  \n",
      "   - Table 7 (Page 27) provides detailed comparisons.  \n",
      "\n",
      "2. **Financial NLP Tasks**:  \n",
      "   - **Sentiment Analysis (SA)**: Critical for market prediction and trading strategies.  \n",
      "   - **Named Entity Recognition (NER)**: Identifies financial entities like company names and stock tickers.  \n",
      "   - **Question Answering (QA)**: Requires synthesizing information from dense financial reports.  \n",
      "   - **Text Classification (TC)**: Classifies documents into predefined categories (Page 30).  \n",
      "\n",
      "3. **Challenges**: Human evaluation reveals gaps in Flan-PaLM responses, emphasizing the need for instruction tuning to improve clinical alignment [213].  \n",
      "\n",
      "Citations: Pages 27, 30, and 60.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "# Test basic retrieval\n",
    "# ask(\"What are the main methods mentioned in this paper?\")\n",
    "\n",
    "\n",
    "# # Test multi-step reasoning\n",
    "ask(\"First find the methodology, then look for evaluation results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a082a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Agentic RAG Chat - Type 'quit' to exit\n",
      "\n",
      "============================================================\n",
      "Question: what is rag\n",
      "============================================================\n",
      "\n",
      "💬 Answer:\n",
      "what is rag\n",
      "\n",
      "🔧 Using: retrieve_context with {'query': 'what is rag'}\n",
      "🔍 Searching: 'what is rag'\n",
      "✓ Found 4 relevant chunks\n",
      "\n",
      "💬 Answer:\n",
      "Page 122: Benchmarks such as SQuAD [33], Natural Questions [71], and specialized datasets for re-\n",
      "trieval tasks are widely used for assessment.\n",
      "Despite its promise, RAG faces several challenges:\n",
      "1. Retrieval Latency: Efficiently querying large databases in real time remains a technical\n",
      "hurdle.\n",
      "2. Data Quality: The reliability of generated outputs depends heavily on the quality of\n",
      "retrieved data.\n",
      "3. Scalability: Handling large-scale retrieval tasks while maintaining high generation quality\n",
      "is complex.\n",
      "Future research avenues include:\n",
      "• Expanding RAG frameworks to support multi-modal inputs, such as text, images, and\n",
      "audio.\n",
      "• Enhancing retrieval efficiency through novel indexing and search techniques.\n",
      "• Improving integration mechanisms for tighter coupling between retrieval and generation\n",
      "modules.\n",
      "RAG represents a transformative step in LLM development, bridging the gap between static\n",
      "pre-trained knowledge and dynamic, context-aware generation. By combining retrieval and\n",
      "\n",
      "Page 117: hance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\n",
      "like sliding window approach, fine-grained segmentation and metadata. It incorporates\n",
      "additional optimization techniques to streamline the retrieval process [280].\n",
      "3. Modular RAG: this architecture advances beyond previous RAG paradigms (Naive and\n",
      "Advanced RAG) by offering greater adaptability, flexibility, and functionality. It intro-\n",
      "duces new components and interaction patterns to address the challenges of static and\n",
      "rigid retrieval-generation frameworks, making it suitable for diverse tasks and dynamic\n",
      "scenarios. Modular RAG incorporates specialized modules to enhance retrieval and gen-\n",
      "eration:\n",
      "• Search Module: Supports direct searches across diverse data sources such as databases,\n",
      "search engines, and knowledge graphs using LLM-generated queries [303].\n",
      "• RAGFusion: Implements multi-query strategies for diverse perspectives, utilizing\n",
      "parallel searches and re-ranking for knowledge discovery [320].\n",
      "118\n",
      "\n",
      "Page 122: pre-trained knowledge and dynamic, context-aware generation. By combining retrieval and\n",
      "generation, RAG systems are poised to redefine the capabilities of AI in knowledge-intensive\n",
      "tasks.\n",
      "123\n",
      "\n",
      "Page 116: 1. Enhanced Knowledge Integration : By querying external databases, RAG systems\n",
      "continuously update their knowledge base, addressing the limitations of static pre-trained\n",
      "models.\n",
      "2. Improved Accuracy: Retrieved data serves as contextual grounding, reducing halluci-\n",
      "nations and increasing the factual reliability of generated outputs.\n",
      "117\n",
      "\n",
      "💬 Answer:\n",
      "<think>\n",
      "Okay, the user asked \"what is RAG\". From the tool response, I see that RAG is discussed in the context of AI, specifically Retrieval-Augmented Generation. Let me parse the information from the provided pages.\n",
      "\n",
      "On page 116, it mentions Enhanced Knowledge Integration and Improved Accuracy, explaining that RAG systems use external databases to update their knowledge and reduce hallucinations. That's a key point. \n",
      "\n",
      "Page 117 talks about Modular RAG, which includes components like the Search Module and RAGFusion. These modules handle different aspects of retrieval and generation, making the system more adaptable. The sliding window approach and metadata optimization are techniques used here.\n",
      "\n",
      "Page 122 lists challenges like retrieval latency, data quality, and scalability. It also mentions future research directions, such as multi-modal support and better integration between retrieval and generation. The conclusion there emphasizes how RAG bridges static knowledge with dynamic generation.\n",
      "\n",
      "Putting this together, RAG is a framework that combines retrieval of external information with generative models to enhance accuracy and knowledge. The answer should define RAG, mention its components, challenges, and benefits, citing the relevant pages. I need to make sure to reference the page numbers for each point as per the instructions.\n",
      "</think>\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a framework that integrates retrieval systems with generative models to enhance the accuracy and knowledge depth of AI outputs. Here's a structured explanation based on the document:\n",
      "\n",
      "1. **Core Concept**:  \n",
      "   RAG bridges static pre-trained knowledge (e.g., from language models) with dynamic, context-aware generation by incorporating external data during inference. This reduces hallucinations and improves factual reliability [Page 122].\n",
      "\n",
      "2. **Key Components**:  \n",
      "   - **Search Module**: Enables direct queries across diverse data sources (databases, search engines, knowledge graphs) using LLM-generated queries [Page 117].  \n",
      "   - **RAGFusion**: Implements multi-query strategies for diverse perspectives, using parallel searches and re-ranking to enhance knowledge discovery [Page 117].  \n",
      "\n",
      "3. **Benefits**:  \n",
      "   - **Enhanced Knowledge Integration**: Continuously updates knowledge bases via external databases, overcoming limitations of static pre-trained models [Page 116].  \n",
      "   - **Improved Accuracy**: Retrieved data grounds responses in context, reducing hallucinations and increasing factual reliability [Page 116].  \n",
      "\n",
      "4. **Challenges**:  \n",
      "   - **Retrieval Latency**: Efficient real-time querying of large databases remains technically challenging [Page 122].  \n",
      "   - **Data Quality**: Output reliability depends heavily on the quality of retrieved data [Page 122].  \n",
      "   - **Scalability**: Maintaining high generation quality in large-scale retrieval tasks is complex [Page 122].  \n",
      "\n",
      "5. **Future Directions**:  \n",
      "   - Expanding to support multi-modal inputs (text, images, audio).  \n",
      "   - Enhancing retrieval efficiency via novel indexing/search techniques.  \n",
      "   - Tighter integration between retrieval and generation modules [Page 122].  \n",
      "\n",
      "The document highlights RAG as a transformative approach in LLM development, enabling AI systems to dynamically leverage external knowledge for more accurate and contextually relevant responses [Page 122].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m question:\n\u001b[32m     13\u001b[39m             ask(question)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mchat\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🤖 Agentic RAG Chat - Type \u001b[39m\u001b[33m'\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to exit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     question = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mYour question: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m question.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\ipykernel\\kernelbase.py:1459\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1457\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\ipykernel\\kernelbase.py:1504\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1502\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1503\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1504\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1506\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE CHAT\n",
    "# ============================================================================\n",
    "def chat():\n",
    "    \"\"\"Start interactive chat with the agentic RAG.\"\"\"\n",
    "    print(\"\\n🤖 Agentic RAG Chat - Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        if question:\n",
    "            ask(question)\n",
    "\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
