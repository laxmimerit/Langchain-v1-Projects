{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45f918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages:\n",
    "# pip install -U langchain langgraph langchain-chroma langchain-ollama langchain-community pypdf\n",
    "\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b82994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1 Loading PDF Document...\n",
      "Using PyPDFLoader which loads one Document object per PDF page.\n",
      "For each page, we can access:\n",
      "- The string content of the page\n",
      "- Metadata containing the file name and page number\n",
      "\n",
      "✓ Loaded 174 pages from PDF\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: DOCUMENTS AND DOCUMENT LOADERS\n",
    "# ============================================================================\n",
    "print(\"\\n1.1 Loading PDF Document...\")\n",
    "print(\"Using PyPDFLoader which loads one Document object per PDF page.\")\n",
    "print(\"For each page, we can access:\")\n",
    "print(\"- The string content of the page\")\n",
    "print(\"- Metadata containing the file name and page number\")\n",
    "\n",
    "# Load PDF - works with both online URLs and local file paths\n",
    "pdf_url = \"https://arxiv.org/pdf/2501.04040.pdf\"\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(documents)} pages from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d522b35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Document Structure:\n",
      "- Content length: 2140 characters\n",
      "- Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'total_pages': 174, 'page': 0, 'page_label': '1'}\n",
      "- Content preview: A Survey on Large Language Models with some Insights\n",
      "on their Capabilities and Limitations\n",
      "Andrea Matarazzo\n",
      "Expedia Group\n",
      "Italy\n",
      "a.matarazzo@gmail.com\n",
      "Riccardo Torlone\n",
      "Roma Tre University\n",
      "Italy\n",
      "riccard...\n"
     ]
    }
   ],
   "source": [
    "sample_doc = documents[0]\n",
    "print(f\"\\nSample Document Structure:\")\n",
    "print(f\"- Content length: {len(sample_doc.page_content)} characters\")\n",
    "print(f\"- Metadata: {sample_doc.metadata}\")\n",
    "print(f\"- Content preview: {sample_doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc2c1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.1 Configuring Text Splitter...\n",
      "- Chunk size: 4096 characters (as specified)\n",
      "- Overlap: 410 characters (10% overlap)\n",
      "- Method: Recursive character splitting\n",
      "\n",
      "2.2 Splitting documents into chunks...\n",
      "\n",
      "✓ Split 174 pages into 183 chunks\n",
      "\n",
      "Chunk Analysis:\n",
      "- Average chunk size: 2623 characters\n",
      "- Largest chunk: 4079 characters\n",
      "- Smallest chunk: 420 characters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: TEXT SPLITTING  \n",
    "# ============================================================================\n",
    "print(\"\\n2.1 Configuring Text Splitter...\")\n",
    "print(\"- Chunk size: 4096 characters (as specified)\")\n",
    "print(\"- Overlap: 410 characters (10% overlap)\")\n",
    "print(\"- Method: Recursive character splitting\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4096,\n",
    "    chunk_overlap=410,  # 10% of 4096\n",
    "    length_function=len,\n",
    "    add_start_index=True,  # Preserves character index as metadata\n",
    ")\n",
    "\n",
    "print(\"\\n2.2 Splitting documents into chunks...\")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\n✓ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
    "\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(f\"\\nChunk Analysis:\")\n",
    "print(f\"- Average chunk size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\n",
    "print(f\"- Largest chunk: {max(chunk_sizes)} characters\")\n",
    "print(f\"- Smallest chunk: {min(chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e748bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: EMBEDDINGS\n",
    "# ============================================================================\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f2bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.1 Creating Chroma Vector Store...\n",
      "- Collection name: pdf_collection\n",
      "- Storage: Local persistent directory\n",
      "- Embedding function: nomic-embed-text via Ollama\n",
      "✓ Added 183 document chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: VECTOR STORES\n",
    "# ============================================================================\n",
    "print(\"\\n4.1 Creating Chroma Vector Store...\")\n",
    "print(\"- Collection name: pdf_collection\")\n",
    "print(\"- Storage: Local persistent directory\")\n",
    "print(\"- Embedding function: nomic-embed-text via Ollama\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=chunks)\n",
    "\n",
    "print(f\"✓ Added {len(chunks)} document chunks to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582660b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.1 Basic Similarity Search\n",
      "Finding documents most similar to a query using cosine similarity...\n",
      "\n",
      "Query: 'What is the main methods available for RAG?'\n",
      "Retrieved 3 most similar chunks:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: or where the sought information is highly nuanced. IRCoT [223] employs a chain-of-\n",
      "thought (CoT) approach, using retrieval results to iteratively refine the CoT reasoning\n",
      "process. ToC (Tree of Clarifications) [284] systematically addresses ambiguities in queries\n",
      "by constructing clarification trees t...\n",
      "Source: Page 122\n",
      "\n",
      "--- Result 2 ---\n",
      "Content: Figure 65: Technology tree of RAG research. The stages of involving RAG mainly include pre-\n",
      "training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused\n",
      "on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the in-\n",
      "ference...\n",
      "Source: Page 118\n",
      "\n",
      "--- Result 3 ---\n",
      "Content: Figure 64: Final Pass rates of models across LLM Modulo Iterations. Source: Kambhampati et al.\n",
      "[379]\n",
      "3. Domain Adaptability: RAG enables LLMs to integrate domain-specific information,\n",
      "improving performance in specialized areas like law, medicine, and engineering.\n",
      "RAG systems are categorized into thr...\n",
      "Source: Page 117\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: QUERYING THE VECTOR STORE\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n5.1 Basic Similarity Search\")\n",
    "print(\"Finding documents most similar to a query using cosine similarity...\")\n",
    "\n",
    "query = \"What is the main methods available for RAG?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Retrieved {len(results)} most similar chunks:\")\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\")\n",
    "    print(f\"Source: Page {doc.metadata.get('page', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e064db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.2 Similarity Search with Scores\n",
      "Same search but with similarity scores to see confidence levels...\n",
      "\n",
      "--- Result 1 (Similarity Score: 0.9371) ---\n",
      "Content: Figure 65: Technology tree of RAG research. The stages of involving RAG mainly include pre-\n",
      "training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused\n",
      "on lever...\n",
      "Source: Page 118\n",
      "\n",
      "--- Result 2 (Similarity Score: 0.9464) ---\n",
      "Content: probability for a hypothesis as more evidence or information becomes available. Fundamentally, Bayesian\n",
      "inference uses prior knowledge, in the form of a prior distribution in order to estimate posteri...\n",
      "Source: Page 86\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.2 Similarity Search with Scores\")\n",
    "print(\"Same search but with similarity scores to see confidence levels...\")\n",
    "\n",
    "results_with_scores = vector_store.similarity_search_with_score(query, k=2)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n--- Result {i} (Similarity Score: {score:.4f}) ---\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: Page {doc.metadata.get('page', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3 Metadata Filtering\n",
      "Using metadata filters to search specific parts of the document...\n",
      "\n",
      "Available metadata in our chunks:\n",
      "Sample metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'total_pages': 174, 'page': 0, 'page_label': '1', 'start_index': 0}\n",
      "Available page numbers (sample): [0, 1, 2, 3, 4]...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3 Metadata Filtering\")\n",
    "print(\"Using metadata filters to search specific parts of the document...\")\n",
    "\n",
    "# First, let's see what metadata is available\n",
    "print(\"\\nAvailable metadata in our chunks:\")\n",
    "if chunks:\n",
    "    sample_metadata = chunks[0].metadata\n",
    "    print(f\"Sample metadata: {sample_metadata}\")\n",
    "    \n",
    "    # Get unique page numbers for filtering examples\n",
    "    page_numbers = set()\n",
    "    for chunk in chunks[:10]:  # Check first 10 chunks\n",
    "        if 'page' in chunk.metadata:\n",
    "            page_numbers.add(chunk.metadata['page'])\n",
    "    print(f\"Available page numbers (sample): {sorted(list(page_numbers))[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f63776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3.1 Filter by Specific Page\n",
      "Searching only in Page 0:\n",
      "  Result 1: Page 0 - A Survey on Large Language Models with some Insights\n",
      "on their Capabilities and Limitations\n",
      "Andrea Matarazzo\n",
      "Expedia Group\n",
      "Italy\n",
      "a.matarazzo@gmail.com\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3.1 Filter by Specific Page\")\n",
    "if page_numbers:\n",
    "    target_page = sorted(list(page_numbers))[0]  # Use first available page\n",
    "    page_results = vector_store.similarity_search(\n",
    "        \"methodology approach\",\n",
    "        k=2,\n",
    "        filter={\"page\": target_page}\n",
    "    )\n",
    "    print(f\"Searching only in Page {target_page}:\")\n",
    "    for i, doc in enumerate(page_results, 1):\n",
    "        print(f\"  Result {i}: Page {doc.metadata.get('page')} - {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff575a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3.2 Filter by Page Range\n",
      "Searching in pages >= 1:\n",
      "  Result 1: Page 90 - Figure 40: A: Chain of thoughts (in blue) are intermediate reasoning steps towards a final answer.\n",
      "The input of CoT prompting is a stack of a few (oft...\n",
      "  Result 2: Page 111 - Figure 57: Reflexion works on decision-making, programming, and reasoning tasks. Source: Shinn\n",
      "et al. [322]\n",
      "Figure 58: (a) Diagram of Reflexion. (b) R...\n",
      "  Result 3: Page 36 - hℓ = hℓ−1 + SA(LN(hℓ−1))\n",
      "hℓ = hℓ + FFN(LN(hℓ))\n",
      "where SA is multi-head self-attention, LN is layer-normalization, and FFN is a feed-forward net-\n",
      "work w...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3.2 Filter by Page Range\")\n",
    "if len(page_numbers) > 1:\n",
    "    # Filter for pages greater than or equal to a certain page\n",
    "    min_page = sorted(list(page_numbers))[1] if len(page_numbers) > 1 else sorted(list(page_numbers))[0]\n",
    "    range_results = vector_store.similarity_search(\n",
    "        \"results conclusions\",\n",
    "        k=3,\n",
    "        filter={\"page\": {\"$gte\": min_page}}  # Pages >= min_page\n",
    "    )\n",
    "    print(f\"Searching in pages >= {min_page}:\")\n",
    "    for i, doc in enumerate(range_results, 1):\n",
    "        print(f\"  Result {i}: Page {doc.metadata.get('page')} - {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c0b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3.3 Multiple Metadata Filters\n",
      "Using complex filter (page >= 0 AND has source):\n",
      "  Result 1: Page 118 - Figure 65: Technology tree of RAG research. The stages of involving RAG mainly include pre-\n",
      "training, fine-tuning, and inference. With the emergence o...\n",
      "  Result 2: Page 119 - Figure 66: Retrieval-Augmented Generation (RAG) Framework mainly consists of 3 steps. 1) In-\n",
      "dexing. Documents are split into chunks, encoded into vec...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3.3 Multiple Metadata Filters\")\n",
    "# Complex filtering with multiple conditions\n",
    "complex_results = vector_store.similarity_search(\n",
    "    \"research findings\",\n",
    "    k=2,\n",
    "    filter={\n",
    "        \"$and\": [\n",
    "            {\"page\": {\"$gte\": 0}},  # Page 0 or higher\n",
    "            {\"source\": {\"$ne\": \"\"}}  # Has a source\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Using complex filter (page >= 0 AND has source):\")\n",
    "for i, doc in enumerate(complex_results, 1):\n",
    "    print(f\"  Result {i}: Page {doc.metadata.get('page')} - {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f59058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.4 Search by Vector with Filtering\n",
      "Combining vector search with metadata filtering...\n",
      "Vector search in Page 0:\n",
      "  Result 1: A Survey on Large Language Models with some Insights\n",
      "on their Capabilities and Limitations\n",
      "Andrea Matarazzo\n",
      "Expedia Group\n",
      "Italy\n",
      "a.matarazzo@gmail.com\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.4 Search by Vector with Filtering\")\n",
    "print(\"Combining vector search with metadata filtering...\")\n",
    "\n",
    "# Generate embedding for a query and search with filter\n",
    "query_embedding = embeddings.embed_query(\"experimental setup\")\n",
    "if page_numbers:\n",
    "    target_page = sorted(list(page_numbers))[0]\n",
    "    vector_results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=2,\n",
    "        filter={\"page\": target_page}\n",
    "    )\n",
    "    print(f\"Vector search in Page {target_page}:\")\n",
    "    for i, doc in enumerate(vector_results, 1):\n",
    "        print(f\"  Result {i}: {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdb2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Creating Retriever...\n",
      "MMR balances similarity with diversity in retrieved results\n",
      "Score threshold retriever only returns documents above similarity threshold\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: RETRIEVERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6. Creating Retriever...\")\n",
    "\n",
    "# Similarity Retriever\n",
    "similarity_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# MMR Retriever\n",
    "print(\"MMR balances similarity with diversity in retrieved results\")\n",
    "mmr_retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "# Score Threshold Retriever\n",
    "print(\"Score threshold retriever only returns documents above similarity threshold\")\n",
    "threshold_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5, \"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f49c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'What are the key contributions of this paper?'\n",
      "✓ Retrieved 4 relevant document chunks\n",
      "\n",
      "Context that would be sent to LLM:\n",
      "\n",
      "Chunk 1: Figure 56: Overview of the DEPS interactive plannet architecture. Source: Wang et al. [344]\n",
      "as an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan\n",
      "using the descriptor and explainer information. To improve ...\n",
      "\n",
      "Chunk 2: Figure 57: Reflexion works on decision-making, programming, and reasoning tasks. Source: Shinn\n",
      "et al. [322]\n",
      "Figure 58: (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. Source: Shinn et al.\n",
      "[322]\n",
      "process are the notion of short-term an...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: RAG FOUNDATION\n",
    "# ============================================================================\n",
    "final_query = \"What are the key contributions of this paper?\"\n",
    "context_docs = similarity_retriever.invoke(final_query)\n",
    "\n",
    "print(f\"\\nQuery: '{final_query}'\")\n",
    "print(f\"✓ Retrieved {len(context_docs)} relevant document chunks\")\n",
    "\n",
    "# Show what would be sent to LLM\n",
    "print(f\"\\nContext that would be sent to LLM:\")\n",
    "for i, doc in enumerate(context_docs[:2], 1):  # Show first 2 for brevity\n",
    "    print(f\"\\nChunk {i}: {doc.page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef7c7113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='32a679fe-1fbd-4b4f-84d5-278d7aba5e2c', metadata={'subject': '', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'page_label': '111', 'trapped': '/False', 'total_pages': 174, 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': '2025-02-11T01:48:37+00:00', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'keywords': '', 'author': '', 'producer': 'pdfTeX-1.40.25', 'page': 110, 'start_index': 0}, page_content='Figure 56: Overview of the DEPS interactive plannet architecture. Source: Wang et al. [344]\\nas an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan\\nusing the descriptor and explainer information. To improve the feasibility of generated plans\\nconditioned on the current state, which is the second identified challenge, Wang et al. [344] use\\na learned goal-selector to choose the most accessible sub-task based on the proximity to each\\ncandidate sub-goal. Developing multi-task agents that can accomplish a vast and diverse suite\\nof tasks in complex domains has been considered a key milestone towards generally capable\\nartificial intelligence.\\nMemorization Long-term memory is a crucial component in the planning process. It allows\\nmodels to store and retrieve information from past experiences and the short-term memory\\ncapabilities provided by in-context learning (ICL) in large language models (LLMs). Reflex-\\nion [322] introduces an innovative framework that enhances language agents through linguistic\\nfeedback rather than weight updates. Reflexion agents reflect verbally on task feedback, main-\\ntaining reflective text in an episodic memory buffer to improve decision-making in subsequent\\ntrials. This process mirrors how humans iteratively learn complex tasks by reflecting on previ-\\nous failures to develop improved strategies for future attempts.\\nReflexion can incorporate various types (scalar values or free-form language) and sources\\n(external or internally simulated) of feedback signals, significantly improving performance over\\na baseline agent across diverse tasks such as sequential decision-making, coding, and language\\nreasoning.\\nThe Reflexion framework consists of four main components: the Actor, the Evaluator, the\\nSelf-Reflection model, and the memory. The Actor, built upon an LLM, is specifically prompted\\nto generate necessary text and actions based on state observations. The Evaluator assesses the\\nquality of the Actor’s outputs by computing a reward score that reflects performance within the\\ngiven task context. The Self-Reflection model, also instantiated as an LLM, generates verbal\\nself-reflections to provide valuable feedback for future trials. Core components of the Reflexion\\n111'),\n",
       " Document(id='1c94158f-af27-41ac-b67c-ed8bc8eacfaa', metadata={'page_label': '112', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creationdate': '2025-02-11T01:48:37+00:00', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'author': '', 'trapped': '/False', 'subject': '', 'creator': 'LaTeX with hyperref', 'title': '', 'start_index': 0, 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'page': 111, 'producer': 'pdfTeX-1.40.25', 'total_pages': 174}, page_content='Figure 57: Reflexion works on decision-making, programming, and reasoning tasks. Source: Shinn\\net al. [322]\\nFigure 58: (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. Source: Shinn et al.\\n[322]\\nprocess are the notion of short-term and long-term memory. At inference time, the Actor\\nconditions its decisions on short and long-term memory, similar to how humans remember fine-\\ngrain recent details while also recalling distilled meaningful experiences from long-term memory.\\nIn the RL setup, the trajectory history serves as the short-term memory, while outputs from\\nthe Self-Reflection model are stored in long-term memory. These two memory components\\nwork together to provide specific context. Still, they are also influenced by lessons learned over\\nseveral trials, a key advantage of Reflexion agents over other LLM action choice works. Given a\\nsparse reward signal, such as a binary success status (success/fail), the current trajectory, and\\nits persistent memory mem, the self-reflection model generates nuanced and specific feedback.\\nThis feedback, which is more informative than scalar rewards, is then stored in the agent’s\\nmemory mem. For example, in a multi-step decision-making task, if the agent receives a\\nfailure signal, it can infer that a specific action ai led to subsequent incorrect actions ai+1 and\\n112'),\n",
       " Document(id='69314b86-147e-4d20-bcbf-f8ed2d0ea890', metadata={'creator': 'LaTeX with hyperref', 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'title': '', 'page_label': '118', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': '2025-02-11T01:48:37+00:00', 'start_index': 0, 'producer': 'pdfTeX-1.40.25', 'creationdate': '2025-02-11T01:48:37+00:00', 'trapped': '/False', 'total_pages': 174, 'subject': '', 'page': 117, 'author': '', 'keywords': ''}, page_content='Figure 64: Final Pass rates of models across LLM Modulo Iterations. Source: Kambhampati et al.\\n[379]\\n3. Domain Adaptability: RAG enables LLMs to integrate domain-specific information,\\nimproving performance in specialized areas like law, medicine, and engineering.\\nRAG systems are categorized into three main paradigms:\\n1. Na¨ ıve RAG: it was the first iteration of RAG systems. It follows the traditional pipeline\\nof indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read”\\nframework [304]. This approach is simple and effective but suffers notable drawbacks in\\nterms of retrieval precision (e.g., missing crucial information) and generation accuracy\\n(e.g., allowing for hallucinations, toxicity or bias).\\n2. Advanced RAG: it introduces specific improvements to address the limitations of Na¨ ıve\\nRAG. About retrieval quality, it employes pre-retrieval and post-retrieval strategies to en-\\nhance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\\nlike sliding window approach, fine-grained segmentation and metadata. It incorporates\\nadditional optimization techniques to streamline the retrieval process [280].\\n3. Modular RAG: this architecture advances beyond previous RAG paradigms (Naive and\\nAdvanced RAG) by offering greater adaptability, flexibility, and functionality. It intro-\\nduces new components and interaction patterns to address the challenges of static and\\nrigid retrieval-generation frameworks, making it suitable for diverse tasks and dynamic\\nscenarios. Modular RAG incorporates specialized modules to enhance retrieval and gen-\\neration:\\n• Search Module: Supports direct searches across diverse data sources such as databases,\\nsearch engines, and knowledge graphs using LLM-generated queries [303].\\n• RAGFusion: Implements multi-query strategies for diverse perspectives, utilizing\\nparallel searches and re-ranking for knowledge discovery [320].\\n118'),\n",
       " Document(id='9755ed2d-1c45-40d2-913a-192f5d2d3b22', metadata={'creationdate': '2025-02-11T01:48:37+00:00', 'moddate': '2025-02-11T01:48:37+00:00', 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'title': '', 'page': 113, 'trapped': '/False', 'keywords': '', 'start_index': 0, 'source': 'https://arxiv.org/pdf/2501.04040.pdf', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '114', 'total_pages': 174, 'subject': ''}, page_content='Figure 60: Adding and retrieving skills from the skill library in Voyager. Source: Sun et al. [324]\\nFigure 61: Overview of MemoryBank. The memory storage stores past conversations, summarized\\nevents and user portraits, while the memory updating mechanism updates the memory storage. Memory\\nretrieval recalls relevant memory. Source: Zhong et al. [367]\\nBank [367] incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting\\nCurve theory.112 This mechanism allows the model to forget less relevant information and re-\\ntain more important information based on time elapsed and relative relevance, thereby offering\\na human-like memory management system.\\n4.4.5 LLM-modulo Framework\\nLLM-modulo framework are a novel approach to planning that combines the strengths of LLMs\\nwith the modularity of traditional planning systems.\\nThe reasons behind the development of the LLM-modulo framework are manifold. Kamb-\\n112The Ebbinghaus Forgetting Curve is a psychological theory that describes how information is lost over time\\nwhen there is no attempt to retain it. It shows that humans tend to halve their memory of newly learned\\nknowledge in days or weeks unless they consciously review the learned material.\\n114')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757488d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5ddad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7079f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f22595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
